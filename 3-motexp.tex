%\documentclass[0-main.tex]{subfiles}
%\begin{document}

\section{A Motivating Experiment}
We first motivate the study of history-dependent reward learning by a simple illustrative example in RL.
Suppose, we have a task where the goal is to move an agent to a certain state $s_f$ from an initial state $s_0$.
If a task is segmented into smaller subtasks and rewards are placed at the endpoints of the subtasks to guide the agent to overall goal (e.g., $s_1,...,s_k$), counter-intuitively the convergence of the segmented task can sometimes be slower.

\subsection{RC Car Domain Experiment}
\todo{fill in}

\subsection{Explanation for Divergence}
For some tasks, the apparent state representation results in a formulation where rewards may depend on history.
Consider a simplified version of the above task where a robot must reach a location B only after passing through location A.
The natural solution would be to place a reward as A to guide the robot towards location A.
However, when we try to use this reward to learn a policy, we could get a counter-intuitive result where the agent moves from the initial state and stops at A and repeatedly returns to A to collect the reward.
This behavior is not what the operator intends and is because the process is inherently history dependent.
Once the reward at A is confirmed, no more reward can be achieved by returning to this state.

%\end{document}