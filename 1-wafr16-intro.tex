\section{Introduction}
Learning from Demonstrations (LfD) is an effective approach to derive a robotic controller from observations of an expert supervisor~\cite{argall2009survey}.
One of the primary challenges in LfD is generalization, where the learned policy must extrapolate from the observed demonstrations to new instances of the same task.
Generalization can be difficult with a purely observational approach (i.e., no physical rollouts) due to differences between the demonstration and execution dynamics~\cite{abbeel2004apprenticeship}, modeling errors~\cite{ross2011reduction}, or simply because the demonstrations are noisy or inconsistent~\cite{krishnan2015tsc}.
Prior work on \emph{apprenticeship learning}~\cite{DBLP:conf/nips/KolterAN07, coates2008learning, abbeel2004apprenticeship} argues for a two-phase approach that first applies Inverse Reinforcement Learning (IRL) to infer the supervisor's implicit reward function, and then optimizes for this reward function using an policy search approach like Reinforcement Learning (RL)~\cite{ng2000algorithms, abbeel2004apprenticeship}.


Apprenticeship learning faces the same challenges with long time horizons as \emph{ab initio} RL.
Learned reward functions can be delayed, where the consequences of an action are only observed long after it is taken, leading to challenges in assigning credit.
This is particularly an issue in domains such as assembly or robotic surgery where a robot must reach a sequence of precise state-space goals in a particular order for the task to be successful.
The motivation of this paper is to explore techniques to scale such algorithms to longer time horizons.
The key insight is to divide the task into segments with local reward functions that ``build up'' to the final goal.

We propose \hirlfull (\hirl), which is a three phase algorithm, that c: (1) unsupervised trajectory segmentation, (2) local inverse reinforcement learning, and (3) policy learning.
For each one of these three phases, we characterize 






To address this problem, one approach is to divide the task into segments with local reward functions that ``build up'' to the final goal.
In existing work on multi-step IRL, this sequential structure is defined manually~\cite{DBLP:conf/nips/KolterAN07}.
We propose an approach that automatically learns sequential structure and assigns local reward functions to segments.
The combined problem is nontrivial because solving $k$ independent problems neglects the shared structure in the value function during the policy learning phase (e.g., a common failure state).
However, jointly optimizing over the segmented problem inherently introduces a dependence on history, namely, any policy must complete step $i$ before step $i+1$.
This potentially leads to an exponential overhead of additional states. 

\hirlfull (\hirl) is based on a model for sequential tasks that represents them as a sequence of reward functions $\mathbf{R}_{seq}=[R_1,...,R_k]$ and transition regions (subsets of the state-space) $G = [\rho_1, ...,\rho_k]$ such that $R_1$ is the reward function until $\rho_1$ is reached, after which $R_2$ becomes the reward and so on.
\hirl assumes that demonstrations are locally optimal (as in IRL), and all demonstrations reach each $\rho \in G$ in the same sequence.
In the first phase of the algorithm, \hirl segments the demonstrations and infers the transition regions using a kernelized variant of an algorithm proposed in our prior work~\cite{krishnan2015tsc,murali2016}.
In the second phase, \hirl uses the inferred transition regions to segment the set of demonstrations and applies IRL locally to each segment to construct the sequence of reward functions $\mathbf{R}_{seq}$.
Once these rewards are learned, \hirl computes a policy using an RL algorithm (Q-Learning) over an augmented state-space that indicates the sequence of previously reached reward transition regions. We show that this augmentation has an additional space complexity independent of the state-space and linear in the number of rewards.

\vspace{3pt}
\noindent \textbf{Our contributions are:}
\begin{enumerate}[
    topsep=0pt,
    noitemsep,
    % partopsep=1ex,
    % parsep=1ex,
    leftmargin=*,
    % itemindent=3ex
    ]
\item A three-phase algorithm, \hirl, to learn policies for sequential robot tasks.
\item We extend the Transition State Clustering algorithm~\cite{krishnan2015tsc,murali2016} with kernelization for segmentation that is robust to local non-linear (but smooth) dynamics.
\item A novel state-space augmentation to enforce sequential dependencies using binary indicators of the previously completed segments, which can be efficiently stored and computed based on the first phase of \hirl.
\item Simulation and physical experiments comparing \hirl with Supervised Learning and MaxEnt-IRL.
% Experiments suggest that \hirl can generalize from the demonstrations to unseen examples and is robust to noise in the environment and initial conditions. 
\end{enumerate}




%\todo{Add numbers}
