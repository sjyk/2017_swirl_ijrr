\section{Introduction}
Learning from Demonstrations (LfD) is an effective approach to derive a robotic controller from observations of an expert supervisor~\cite{argall2009survey}.
One of the primary challenges in LfD is generalization, where the learned policy must extrapolate from the observed demonstrations to new instances of the same task.
Generalization can be difficult with a purely observational approach (i.e., no physical rollouts) due to differences between the demonstration and execution dynamics~\cite{abbeel2004apprenticeship}, modeling errors~\cite{ross2011reduction}, or because the demonstrations are noisy or inconsistent~\cite{krishnan2015tsc}.
Prior work on \emph{apprenticeship learning}~\cite{DBLP:conf/nips/KolterAN07, coates2008learning, abbeel2004apprenticeship} argues for a two-phase approach that first applies Inverse Reinforcement Learning (IRL) to infer the supervisor's implicit reward function, and then optimizes for this reward function using an policy search approach like Reinforcement Learning (RL)~\cite{ng2000algorithms, abbeel2004apprenticeship}.

However, apprenticeship learning faces the same challenges in scaling to tasks with long time horizons as \emph{ab initio} RL.
Learned reward functions can be delayed, where the consequences of an action are only observed long after it is taken, leading to difficulties in assigning credit.
This reward function might be very difficult to directly optimize, in terms of sample-complexity, for every new task instance.
Such reward functions are inherrent to domains such as assembly or robotic surgery where a robot must reach a sequence of precise state-space goals in a particular order for the task to be successful.
 
Recently, there has been significant interest in unsupervised segmentation algorithms that partition a task into a sequence of temporally extended steps and correspond them across demonstrations~\citep{barbivc2004segmenting, chiappa2010movement,  alvarez2010switched,calinon2010learning, kruger2012imitation, niekum2012learning, wachter2015hierarchical, lee2015autonomous}.
Segmentation has long been a part of LfD~\citep{argall2009survey}, since it facilitates learning localized control policies~\citep{murali2015learning, niekum2012learning, konidaris2011robot}, adaptation to unseen scenarios~\citep{ijspreet2002learning, ude2010task}, and demonstrator skill-assessment~\citep{reiley2010motion, gao2014jigsaws}.
However, existing work in the IRL/apprenticeship learning setting has only considered manually designed segments~\cite{DBLP:conf/nips/KolterAN07}.
This paper explores combining unsupervised trajectory segmention with IRL, and whether this combination can reduce the sample-complexity of long-horizon tasks.

\hirlfull (\hirl) models a task as a sequence of quadratic rewards $\mathbf{R}_{seq}=[R_1,...,R_k]$ and transition regions (subsets of the state-space) $G = [\rho_1, ...,\rho_k]$ such that $R_1$ is the reward function until $\rho_1$ is reached, after which $R_2$ becomes the reward and so on.
\hirl assumes that demonstrations are locally optimal (as in IRL), and all demonstrations reach each $\rho \in G$ in the same sequence.
\hirl identifies segments in each demonstration that correspond to when a reward is active.
Then, \hirl uses the segmented trajectories to infer both the reward functions $\mathbf{R}_{seq}$ and the transition regions $G$ from the demonstration trajectories.
Finally, \hirl learns a policy over the sequence of local rewards using RL.

Adding segmentation to the IRL problem introduces some additional challenges, and part of the contribution of this work is to formalize these issues.
The segmentation algorithm must preserve the Markov structure of the problem, that is, it is possible to determine whether a segment transition has occured based only only the state and the current segment.
Learning a policy from segmented rewards is nontrivial because solving $k$ independent problems neglects any shared structure in the value function during the policy learning phase (e.g., a common failure state).
However, jointly optimizing over the segmented problem inherently introduces a dependence on history, namely, any policy must complete step $i$ before step $i+1$.
Learning a memory-dependent policy could lead to an exponential overhead of additional states. 

\hirl addresses these challenges by using a variant of a segmentation algorithm proposed in our prior work~\cite{krishnan2015tsc,murali2016}, called Transition State Clustering (TSC). The basic insight of TSC is that a number of trajectory segmentation models (e.g., direction changes, clustering, model-based) were sensitive to noise, leading to inconsistent segments across repeated demonstrations.
TSC enhances these models by first using them to identify a set of candidate segment transitions in each trajectory, and then fits a mixture model to the transition states (states at times transitions occur) in terms of kinematic, sensory, and temporal features. 
While our original motivation was to improve the robustness of segmentation algorithms, a useful property is that it learns a probabilistic model for segment transition given the previous segment has completed.
This property allows the model to be evaluated online during RL to indicate to the robot which segment is currently active.
\hirl computes a policy using an RL algorithm (Q-Learning) over an augmented state-space that indicates the sequence of previously reached reward transition regions. We further show that this augmentation has an additional space complexity independent of the state-space and linear in the number of rewards---and not the potential exponential complexity of a full memory-dependent policy.

%\todo{Add numbers}
