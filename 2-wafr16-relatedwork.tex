%\documentclass[0-main.tex]{subfiles}
%\begin{document}
\section{Related Work and Background}

\vspace{0.25em}\noindent\textbf{Apprenticeship Learning: } \cite{abbeel2004apprenticeship} argue that the reward function is often a more concise representation of task than a policy. As such, a concise reward function is more likely to be robust to small perturbations in the task description. 
The downside is that the reward function is not useful on its own, and ultimately a policy must be retrieved. In the most general case, an RL algorithm must be used to optimize for that reward function~\citep{abbeel2004apprenticeship}.
It is well-established that RL problems often converge slowly in complex tasks when rewards are sparse and not ``shaped'' appropriately~\citep{DBLP:conf/icml/NgHR99, DBLP:conf/aaai/JudahFTG14}.
Our work re-visits this two-phase algorithm in the context of sequential tasks and techniques to scale such an approach to longer time horizons.
Related to \hirl, Kolter et al. studied  \emph{Hierarchical Apprenticeship Learning} to learn bipedal locomotion~\citep{DBLP:conf/nips/KolterAN07}, where the algorithm is provided with a hierarchy sub-tasks.
We explore automatically inferring a sequential task structure from data.

\vspace{0.25em}\noindent\textbf{Motion Primitives: }The LfD and planning communities studied the problem of leveraging motion primitives, or libraries of temporally extended action sequences, to improve generalization. 
Dynamic Motion Primitives construct new motions through a composition of dynamical building blocks ~\citep{ijspreet2002learning,pastor2009learning,manschitz2015learning}.
Much of the work in motion primitives considered manually identified segments, but recently, Niekum et al. \citep{niekum2012learning} proposed learning the set of primitives from demonstrations using the Beta-Process Autoregressive Hidden Markov Model (BP-AR-HMM).
Similarly, \cite{calinon2014skills} proposed the task-parametrized movement model with GMMs for automatic action segmentation.
Both Niekum and Calinon considered the motion planning setting in which analytical planning methods are used to solve a task.
To the best of our knowledge, \hirl is the first to consider segmentation in the IRL setting, where the dynamics can be stochastic.

\vspace{0.25em}\noindent\textbf{Segmentation: } Trajectory segmentation is a well-studied area of research dating back to early biomechanics and robotics research.
For example, \cite{viviani1985segmentation} explored using the ``two-thirds'' power law coefficient to determine segment boundaries in handwriting.
\cite{morasso1983three} showed that rhythmic 3d motions of a human arm could be modeled as piecewise linear.
In a seminal paper, \cite{sternad1999segmentation} provided a formal framework for control-theoretic segmentation of trajectories.
\cite{botvinick2009hierarchically} explored the reinforcement learning analog of the control-theoretic models.
Concurrently, temporal-segmentation was developing in the motion capture community~\citep{moeslund2001survey}.
Recently, some Bayesian approaches have been proposed for the segmentation problem~\citep{asfour2006imitation,calinon2004stochastic,kruger2010learning, vakanski2012trajectory,tanwani2016learning}.
One challenge is collecting enough data to employ these techniques and tuning the hyperparameters.
In prior work, we observed that under the assumption that the task is sequential (same order of primitives in each demonstration) the inference could be modeled as a two-level clustering problem~\citep{krishnan2015tsc}.
This results in improved accuracy and robustness for a small number of demonstrations.
Another relevant result is from \cite{ranchod2015nonparametric}, who use an IRL model to define the primitives, in contrast to the problem of learning a policy after IRL.

\vspace{0.25em}\noindent\textbf{Hierarchical Reinforcement Learning: } 
The field of hierarchical reinforcement learning has a long history~\citep{parr98,suttonPS99,barto03} in AI and in the analysis of biological systems~\citep{botvinick08,botvinick2009hierarchically,solway2014optimal,zacksKEH11,whitenFBL06}.
Early work in hierarchical control demonstrated the advantages of hierarchical structures by handcrafting hierarchical policies~\citep{brooks1986robust} and by learning them given various manual specifications: state abstractions~\citep{dayanH92,hengst02,kolterAN07,konidarisB07}, a set of waypoints~\citep{kaelbling93}, low-level skills~\citep{huberG97,baconP15,liaw17composing}, a set of finite-state meta-controllers~\citep{parrR97}, a set of subgoals~\citep{suttonPS99,dietterich00}, or intrinsic reward~\citep{kulkarni2016hierarchical}.
The key abstraction used in hierarchical RL is the ``options'' framework, where sub-skills are represented by local policies, termination conditions, and initialization conditions.
A high-level policy switches between these options and composes them into a larger task policy.
In this framework, per sub-skill reward functions are called sub-goals. \hirl is an algorithm to learn quadratic sub-goals and termination conditions, where the high-level policy is deterministic and sequentially iterates through the sub-skills. 

