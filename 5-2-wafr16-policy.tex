\subsection*{Phase III. Policy Learning}
\seclabel{policy-learning}
In Phase III, given a new task instance, \hirl uses the learned $[\rho_1,...,\rho_k]$ and $\mathbf{R}_{seq}$ as rewards for a Reinforcement Learning algorithm.
In this section, we describe learning a policy $\pi$ given $\mathbf{R}_{seq}$ and $G$.

However, this problem is not trivial since solving $k$ independent problems neglects any shared structure between the local problems (e.g., a common failure state).
Furthermore, simply taking the aggregate of the rewards can lead to inconsistencies since there is nothing enforcing the order of operations.
The key insight is that a single policy can be learned jointly over all segments over a modified problem where the state-space with additional variables that keep track of the previously achieved segments.
To do so, we require an MDP model that also captures the history of the process.


\begin{phase}[t]
\small
\DontPrintSemicolon
\caption{Phase III. Policy Learning \label{alg:tsh3}}
\KwData{Transition States $G$, Reward Sequence $\mathbf{R}_{seq}$, exploration parameter $\epsilon$}

Initialize $Q(\binom{s}{v},a)$ randomly

\ForEach{$iter \in 0,...,I$}{
    Draw $s_0$ from initial conditions
    
    Initialize $v$ to be $[0,...,0]$
    
    Initialize $j$ to be $1$
    
    \ForEach{$t \in 0,...,T$}{
        Choose best action $a$ based on $Q$ or random action w.p $\epsilon$.
        
        Observe Reward $R_{j}$
        
        Update state to $s'$ and $Q$ via Q-Learning update
        
        If $s'$ is $\in  \rho_{j}$ update $v = v + 1$ and $j = j +1$
    }
}

\KwResult{Policy $\pi$}
\end{phase}

\vspace{-15pt}
\subsubsection{MDPs with Memory}
RL algorithms apply to problems that are specified as MDPs.
The challenge is that some sequential tasks may not be MDPs.
For example, attaining a reward at $\rho_i$ depends on knowing that the reward at goal $\rho_{i-1}$ was attained.
In general, to model this dependence on the past requires MDPs whose state-space also includes history.
This MDP is purely conceptual and is never materialized, but illustrates the mathematical point.

Given a finite-horizon MDP $\mathcal{M}$ as defined in Section \ref{sec:back}, we can define an MDP $\mathcal{M}_H$ as follows.
Let $\mathcal{H}$ denote set of all dynamically feasible sequences of length $\le T$ comprised of the elements of $S$.
Therefore, for an agent at any time $t$, there is a sequence of previously visited states $H_t \in \mathcal{H}$.
The MDP $\mathcal{M}_H$ is defined as:
\[
\mathcal{M}_H = \langle S \times \mathcal{H},A,P'(\cdot,\cdot), R(\cdot,\cdot),T \rangle.
\]
For this MDP, $P'$ not only defines the transitions from the current state $s \mapsto s'$, but also increments the history sequence $H_{t+1} = H_{t} \sqcup s$.
Accordingly, the parametrized reward function $R$ is defined over $S$, $A$, and $H_{t+1}$.

$\mathcal{M}_H$ allows us to address the sequentiality problem since the reward is a function of the state and the history sequence.
However, without some sort of a parametrization of $H_t$, directly solving this MDPs with RL is impractical since it adds an overhead of $\mathbf{O}(e^{T})$ states.

\vspace{-15pt}
\subsubsection{Policy Learning}
Using the sequential task definition, we know that the reward transitions ($R_{i}$ to $R_{i+1}$) only depend on an arrival at the transition state $\rho_{i}$ and not any other aspect of the history.
Therefore, we can store an additional state $v$ is an integer $\{0,...,k\}$ that indicates whether a transition state $i \in 0,...,k$ has been reached.
This vector can be efficiently incremented when the current state $s \in \rho_{i+1}$.
Then, additional complexity of representing the reward with history over $S \times  \{0,...,k\}$ is only $\mathbf{O}(k)$ instead of exponential in the time horizon.
Of course, this result is a consequence of our restriction of the class of tasks.
The sequential task model enforces that the only history required is knowledge of the arrival at sub-goals.

The result is an augmented state-space $\binom{s}{v}$ to account for previous progress.
Over this state-space, we can apply Reinforcement Learning algorithms to iteratively converge to a successful policy for a new task instance.
\hirl applies Q-Learning with an Radial Basis Function value function representation to learn a policy $\pi$ over this state-space and the reward sequence $\mathbf{R}_{seq}$.
This is summarized in Algorithm~\ref{alg:tsh3}.
