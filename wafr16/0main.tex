 \documentclass{llncs}
 %DRAFT MODE preserves formatting otherwise; %comment out
% \usepackage[draft]{graphics}

\let\labelindent\relax
\input{preamble}

\newenvironment{phase}[1][htb]
  {\renewcommand{\algorithmcfname}{Algorithm}% Update algorithm name
   \begin{algorithm}[#1]%
  }{\end{algorithm}}


% Macro for Algorithm Name 
\newcommand{\hirl}{SWIRL\xspace}
\newcommand{\HIRL}{SWIRL\xspace}
\newcommand{\hirlfull}{Sequential Windowed Inverse Reinforcement Learning\xspace}
\newcommand{\tsh}{TSH\xspace}
\newcommand{\TSH}{TSH\xspace}
\newcommand{\tshfull}{Transition State Hashing\xspace}
\newcommand{\tsco}{TSC\xspace}
\newcommand{\sys}{\textsf{TSC+VIS}\xspace}

\newcommand{\tsce}{\texttt{TSC-Endpoint}\xspace}
\newcommand{\tsci}{\texttt{TSC-IRL}\xspace}

\newboolean{include-notes}
\setboolean{include-notes}{true}
\newcommand{\fp}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{FP: #1}}}{}}
%===============================================================

\renewcommand\bibsection
  {\section*{\refname}\small\renewcommand\bibnumfmt[1]{##1.}}
 

\title{\LARGE \bf \hirl: A \hirlfull Algorithm for Robot Tasks With Delayed Rewards}

\author{%
Sanjay Krishnan, 
Animesh Garg, 
Richard Liaw,
Brijen Thananjeyan,\\
Lauren Miller,
Florian T. Pokorny$^{*}$,
Ken Goldberg%, 
% \thanks{EECS \& IEOR, University of California, Berkeley CA USA; \texttt{\{sanjaykrishnan, animesh.garg, rliaw, bthananjeyan, ftpokorny, laurenm, goldberg\}@berkeley.edu}}%
}
\institute{The AUTOLAB at UC Berkeley (automation.berkeley.edu)\\$^{*}$CAS/CVAP, KTH Royal Institute of Technology, Stockholm, Sweden}

%\IEEEoverridecommandlockouts %to enable thanks to appear

% ----------------------------------------------------------
% --- SPACING OVERRIDES ---
% ----------------------------------------------------------
\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{2pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{2pt}
\setlength{\belowcaptionskip}{-10pt}
\linespread{0.95}
% \onehalfspacing
\selectfont

%fix spacing after floats
% \setlength{\textfloatsep}{8pt}% Remove \textfloatsep

\setlength\floatsep{1.25\baselineskip plus 3pt minus 2pt}
\setlength\textfloatsep{1.25\baselineskip plus 3pt minus 2pt}
\setlength\intextsep{1.25\baselineskip plus 3pt minus 2 pt}

% ----------------------------------------------------------
\begin{document}
\maketitle

\begin{abstract}
Inverse Reinforcement Learning (IRL) allows a robot to generalize from demonstrations to previously unseen scenarios by learning the demonstrator's reward function.
However, in multi-step tasks, the learned rewards might be delayed and hard to directly optimize.
We present \hirlfull (\hirl), a three-phase algorithm that partitions a complex task into shorter-horizon subtasks based on linear dynamics transitions that occur consistently across demonstrations.
\hirl then learns a sequence of local reward functions that describe the motion between transitions. 
Once these reward functions are learned, \hirl applies Q-learning to compute a policy that maximizes the rewards. 
We compare \hirl (demonstrations to segments to rewards) with Supervised Policy Learning (SPL - demonstrations to policies) and Maximum Entropy IRL (MaxEnt-IRL demonstrations to rewards) on standard Reinforcement Learning benchmarks: Parallel Parking with noisy dynamics, Two-Link acrobot, and a 2D GridWorld.
We find that \hirl converges to a policy with similar success rates (60\%) in 3x fewer time-steps than MaxEnt-IRL, and requires 5x fewer demonstrations than SPL.
In physical experiments using the da Vinci surgical robot, we evaluate the extent to which \hirl generalizes from linear cutting demonstrations to cutting sequences of curved paths.
\end{abstract} 

\input{1-wafr16-intro.tex}
\input{2-wafr16-relatedwork.tex}
\input{3-wafr16-background.tex}
\input{4-wafr16-segmentation.tex}
%\input{3-model.tex}
%\subfile{3-problemsetup.tex}
%\input{4-segmentation.tex}
%\input{5-embedding.tex}
\input{5-wafr16-reward.tex}
% \input{6-wafr16-policy.tex}
\input{6-wafr16-experiments.tex}
\input{7-wafr16-conclusion.tex}

\vspace{0.5em}

{\footnotesize 
\noindent \textbf{Acknowledgements:}
This research was performed at the AUTOLAB at UC Berkeley in
affiliation with the AMP Lab, BAIR, and the CITRIS "People and Robots" (CPAR) Initiative in affiliation with UC Berkeley's Center for Automation and Learning for Medical Robotics (Cal-MR). The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, and by Google, UC Berkeley's Algorithms, Machines, and People Lab, Knut \& Alice Wallenberg Foundation, and by a major equipment grant from Intuitive Surgical and by generous donations from Andy Chou and Susan and Deepak Lim. We thank our colleagues and the anonymous WAFR reviewers who provided valuable feedback and suggestions, in particular, Pieter Abbeel, Anca Dragan, and Roy Fox.}


%\href{http://j.mp/v-tsc}{j.mp/v-tsc}

\bibliographystyle{splncs}
\bibliography{deepP2P}

%\appendix
%\input{appendix.tex}

\end{document}