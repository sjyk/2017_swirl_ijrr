\subsection{Embedding For Segmentation}\label{sec:kern}
The interesting part about the proposed model is that we do not need to learn the underlying dynamical parameters to identify $G$; we only have to detect that the locations at which the system's dynamics have switched.
This is crucial when we have a small number of demonstrations because we avoid the data requirements of full system identification and can solve a substantially simpler transition detection problem~\cite{willsky2009sharing,niekum2012learning, krishnan2015tsc,murali2016}.
However, it all seems to rest on a seemingly strong assumption about local-linearity.
We can relax this assumption with a kernel embedding of the trajectories.

Let $\Omega = \{\omega_i\}$ be an indexed set of all $\omega_t$ over all demonstrations.
Let $\mathbf{\kappa}(\omega_0,\omega_1)$ define a kernel function over the set $\Omega$.
For example, if $\mathbf{\kappa}$ is the radial basis function (RBF), then:
$ \mathbf{\kappa}(\omega_0,\omega_1) = e^{\frac{-\|\omega_0-\omega_1\|_2^2}{2\sigma}}$.
$\mathbf{\kappa}$ naturally defines a matrix $M$ where:
\[
M_{ij} = \mathbf{\kappa}(\omega_i,\omega_j). 
\]
The top $p'$ eigenvalues define a new embedded feature vector for each $\omega$ in $\mathbb{R}^{p'}$.
In this embedded space, we can apply our transition identification procedure.
This procedure allows us to model non-linearities and states with different scaling properties.

