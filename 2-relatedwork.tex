%\documentclass[0-main.tex]{subfiles}
%\begin{document}
\section{Preliminaries and Related Work}

\subsection{Background}\label{sec:back}
A finite-horizon Markov Decision Process (MDP) can be specified as $\mathcal{M} = \langle S,A,P(\cdot,\cdot), R(\cdot,\cdot),T \rangle$, where $S$ is the state-space, $A$ is the action space, $P: S \times A \mapsto Pr(S)$ is the transition function that maps states and actions to a probability density over subsequent states, $R: S \times A \mapsto \mathbb{R}$ is a reward function over the state and action space, and $T$ is the time-horizon.

Given some distribution over initial states $p_0$, an optimal policy $\pi^*$ is a policy that maximizes the expected reward:
\[
\pi^* = \arg\max_{\pi} \mathbf{E}_{d \sim \pi, p_0} [\sum_{t=0}^T R(s_t,a_t)],
\]
where $d \sim \pi, p_0$ denotes a distribution over the set of all \emph{trajectories} (sequences of state-action tuples) of length $T$ generated by the policy and initial conditions.

\subsection{Hierarchical RL}
Hierarchical RL (HRL) studies solving hierarchies of sub-problems posed as MDPs.
Our problem is a special case of learning hierarchies of MDPs as in HRL. 
The general problem not only considers the dynamics within each subtask but transition dynamics between subtasks.
We formalize this inference problem in this paper, but defer the algorithmic discussion to future work.
HRL considers a process, possibly stochastic, which transitions between a set of MDPs $\{\mathcal{M}_1,...,\mathcal{M}_k\}$. 

\subsubsection{Options}
Policies can constructed using regular actions and composite sequences of actions called ``options"~\cite{csimcsek2004using,menache2002q,mcgovern2001automatic}. These options can be defined a priori or they can be constructed through the process of exploration. 
The problem of discretizing the action-space is different from inferring local rewards for sub-tasks in \hirl.
Here we can make an analogy between policy learning and IRL in Learning From Demonstrations. Rewards are often argued to be more transferable, concise, and easier to interpret~\cite{DBLP:conf/icml/NgHR99}.
Similarly, decomposing MDPs in terms of sub-tasks with local rewards rather than in terms of composite actions may generalize better.

\subsubsection{Motion Primitives and Skill Learning}
Many of the ideas from HRL have been applied in robotics.
Motion primitives are segments that discretize the \emph{action}-space of a robot, and can facilitate faster convergence in LfD~\cite{ijspreet2002learning,pastor2009learning,manschitz2015learning}.
Furthermore, much of the initial work in motion primitives considered manually identified segments, but recently, Niekum et al. \cite{niekum2012learning} proposed learning the set of primitives from demonstrations using the Beta-Process Autoregressive Hidden Markov Model (BP-AR-HMM).
Calinon et al.~\cite{calinon2014skills} also build on a large corpus of literature of unsupervised skill segmentation including the task-parameterized movement model \cite{calinon2014task}, and GMMs for segmentation \cite{calinon2010learning}.

Recently, the robotics community has adopted some of the ideas from HRL in a field called skill learning \cite{konidaris2011robot}. Konidaris et al. studied largely the same problem proposed in this work, where demonstration trajectories are segmented into ``skills" using standard change point detection algorithms. These skills are used to build policies for complex RL tasks. Konidaris and Kuindersma et al.  studied many variants of this problem \cite{konidaris2009efficient}. 

\subsubsection{Sub-Tasks and State-Space Abstractions}
One of the earliest works in this field is by Kaelbling and Pack~\cite{kaelbling1993hierarchical}, where they proposed a technique to decompose a stochastic environment into Voronoi cells to improve learnability.
Dietrich et al. formalized idea of sub-task as an MDP~\cite{dietterich2000hierarchical}, and proposed an algorithm called MAXQ learning to address the information sharing problem.
McGovern and Barto studied this problem for discrete action and state-spaces where they identify states frequently visited by successful policies and use them to construct subgoals \cite{mcgovern2001automatic}. 
Kolter et al. also studied the problem called ``Hierarchical Apprenticeship Learning'' to learn bipedal locomotion~\cite{DBLP:conf/nips/KolterAN07}.
There is also some work in utlizing multi-task learning for RL~\cite{calandriello2014sparse}.
In \hirl, we explore how we can leverage demonstrations that are possibly spatially and temporally varying to infer such hierarchical structure.

\subsection{Inverse Reinforcement Learning}
In Inverse Reinforcement Learning (IRL) problems, we are given all of $\mathcal{M}$ except for the reward function, and a set of demonstrations $\mathcal{D}=\{d_1,...,d_N\}$ which are trajectories of the optimal policy $\pi^*$ with respect to some reward.
The objective is to infer $R(\cdot,\cdot)$ given the demonstrations~\cite{DBLP:conf/icml/NgHR99,DBLP:conf/aaai/ZiebartMBD08, coates2008learning}.
However, typically, it is impractical to observe enough data to learn the function $R(\cdot,\cdot)$ exactly.
Therefore, we often formulate the problem with parametrized reward functions:
\[
R_{\theta}(s,a) = \phi(f(s,a);\theta).
\]
where $f(s,a)$ is a feature vector in $\mathbb{R}^p$, $\theta$ is a parameter vector from some parameter space $\Theta \subseteq \mathbb{R}^q$, and $\phi$ describes the relationship between the two.
For example, we may restrict ourselves to the class of linear functions of the features:
\[
R_{\theta}(s,a) = f(s,a)^T\theta.
\]

Even with parametrization, the reward learning problem is often under-determined~\cite{DBLP:conf/icml/NgHR99}.
Differently shaped rewards can lead to different convergence rates when the rewards are used to learn policies in forward RL.
For IRL, the delayed reward problem has received some attention~\cite{DBLP:conf/ijcai/MacGlashanL15, DBLP:conf/aaai/JudahFTG14}, and in \hirl we consider sequential hierarchies of subtasks as well as reward learning.

\iffalse
\subsection{Memory in ``Forward'' RL}
The problem that we study is highly related to the use of memory states in RL problems.
The addition of memory states is an increasingly popular solution to partial observability in forward RL problems~\cite{DBLP:journals/corr/ZhangLMFA15, DBLP:journals/corr/HeessHLS15}. The empirical success of these techniques is significant enough that it is argued that RL with memory networks sidesteps many of the known hardness of POMDPs~\cite{DBLP:journals/corr/ZhangLMFA15}.
Some of these ideas have been in the RL and Robotics communities for many years (e.g., \cite{DBLP:conf/nips/Bakker01,DBLP:journals/neco/CleeremansSM89,DBLP:journals/neco/Pearlmutter89}).
To the best of our knowledge, there are few examples of such approaches in recent Inverse Reinforcement Learning literature.
\fi
