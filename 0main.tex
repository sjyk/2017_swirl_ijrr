\documentclass[Afour,sageh,times]{sagej}
 %DRAFT MODE preserves formatting otherwise; %comment out
% \usepackage[draft]{graphics}

\let\labelindent\relax
\input{preamble}



\newenvironment{phase}[1][htb]
  {\renewcommand{\algorithmcfname}{Algorithm}% Update algorithm name
   \begin{algorithm}[#1]%
  }{\end{algorithm}}


% Macro for Algorithm Name 
\newcommand{\hirl}{SWIRL\xspace}
\newcommand{\HIRL}{SWIRL\xspace}
\newcommand{\hirlfull}{Sequential Windowed Inverse Reinforcement Learning\xspace}
\newcommand{\tsh}{TSH\xspace}
\newcommand{\TSH}{TSH\xspace}
\newcommand{\tshfull}{Transition State Hashing\xspace}
\newcommand{\tsco}{TSC\xspace}
\newcommand{\sys}{\textsf{TSC+VIS}\xspace}

\newcommand{\tsce}{\texttt{TSC-Endpoint}\xspace}
\newcommand{\tsci}{\texttt{TSC-IRL}\xspace}

\newboolean{include-notes}
\setboolean{include-notes}{true}
\newcommand{\fp}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{FP: #1}}}{}}
%===============================================================

\renewcommand\bibsection
  {\section*{\refname}\small\renewcommand\bibnumfmt[1]{##1.}}
 

\title{\LARGE \bf \hirl: A \hirlfull Algorithm for Robot Tasks With Delayed Rewards}

\author{%
Sanjay Krishnan, 
Animesh Garg, 
Richard Liaw,
Brijen Thananjeyan,\\
Lauren Miller,
Florian T. Pokorny$^{*}$,
Ken Goldberg%, 
}

\affiliation{\affilnum{1}AUTOLAB UC Berkeley\\
\affilnum{2}KTH Sweden}

\corrauth{Sanjay Krishnan}


%\IEEEoverridecommandlockouts %to enable thanks to appear

% ----------------------------------------------------------
% --- SPACING OVERRIDES ---
% ----------------------------------------------------------

% \onehalfspacing
\selectfont

%fix spacing after floats
% \setlength{\textfloatsep}{8pt}% Remove \textfloatsep


% ----------------------------------------------------------
\begin{document}

\begin{abstract}
\textbf{SK. TODO revise}
Inverse Reinforcement Learning (IRL) allows a robot to generalize from demonstrations to previously unseen scenarios by learning the demonstrator's reward function.
However, in multi-step tasks, the learned rewards might be delayed and hard to directly optimize.
We present \hirlfull (\hirl), a three-phase algorithm that partitions a complex task into shorter-horizon subtasks that occur consistently across demonstrations.
\hirl then learns a sequence of local reward functions that describe the motion between transitions. 
Once these reward functions are learned, \hirl applies Q-learning to compute a policy that maximizes the rewards. 
We formalize requirements on the segmentation algorithm that make this possible and evaluate \hirl in a number of discrete and continuous RL domains.
In physical experiments using the da Vinci surgical robot, we evaluate the extent to which \hirl generalizes from linear cutting demonstrations to cutting sequences of curved paths and can help learn multi-step policies for deformable tensioning.
\end{abstract} 

\maketitle



\input{1-wafr16-intro.tex}
\input{2-wafr16-relatedwork.tex}
\input{3-wafr16-background.tex}
\input{4-wafr16-segmentation.tex}
\input{5-wafr16-reward.tex}
\input{5-2-wafr16-policy.tex}
\input{6-wafr16-experiments.tex}
\input{7-wafr16-conclusion.tex}

\vspace{0.5em}

{\footnotesize 
\noindent \textbf{Acknowledgements:}
This research was performed at the AUTOLAB at UC Berkeley in
affiliation with the AMP Lab, BAIR, and the CITRIS "People and Robots" (CPAR) Initiative in affiliation with UC Berkeley's Center for Automation and Learning for Medical Robotics (Cal-MR). The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, and by Google, UC Berkeley's Algorithms, Machines, and People Lab, Knut \& Alice Wallenberg Foundation, and by a major equipment grant from Intuitive Surgical and by generous donations from Andy Chou and Susan and Deepak Lim. We thank our colleagues and the anonymous WAFR reviewers who provided valuable feedback and suggestions, in particular, Pieter Abbeel, Anca Dragan, and Roy Fox.}


%\href{http://j.mp/v-tsc}{j.mp/v-tsc}

\bibliographystyle{SageV}
\bibliography{deepP2P,gmm,gmm2,gmm3}

%\appendix
%\input{appendix.tex}

\end{document}