\documentclass[Afour,sageh,times]{sagej}
 %DRAFT MODE preserves formatting otherwise; %comment out
% \usepackage[draft]{graphics}

\let\labelindent\relax
\input{preamble}



\newenvironment{phase}[1][htb]
  {\renewcommand{\algorithmcfname}{Algorithm}% Update algorithm name
   \begin{algorithm}[#1]%
  }{\end{algorithm}}


% Macro for Algorithm Name 
\newcommand{\hirl}{SWIRL\xspace}
\newcommand{\HIRL}{SWIRL\xspace}
\newcommand{\hirlfull}{Sequential Windowed Inverse Reinforcement Learning\xspace}
\newcommand{\tsh}{TSH\xspace}
\newcommand{\TSH}{TSH\xspace}
\newcommand{\tshfull}{Transition State Hashing\xspace}
\newcommand{\tsco}{TSC\xspace}
\newcommand{\sys}{\textsf{TSC+VIS}\xspace}

\newcommand{\tsce}{\texttt{TSC-Endpoint}\xspace}
\newcommand{\tsci}{\texttt{TSC-IRL}\xspace}

\newboolean{include-notes}
\setboolean{include-notes}{true}
\newcommand{\fp}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{FP: #1}}}{}}
%===============================================================

\renewcommand\bibsection
  {\section*{\refname}\small\renewcommand\bibnumfmt[1]{##1.}}
 

\title{\LARGE \bf \hirl: A \hirlfull Algorithm for Robot Tasks With Delayed Rewards}

\author{%
Sanjay Krishnan, 
Animesh Garg, 
Richard Liaw,
Brijen Thananjeyan,\\
Lauren Miller,
Florian T. Pokorny$^{*}$,
Ken Goldberg%, 
}

\affiliation{\affilnum{1}AUTOLAB UC Berkeley\\
\affilnum{2}KTH Sweden}

\corrauth{Sanjay Krishnan}


%\IEEEoverridecommandlockouts %to enable thanks to appear

% ----------------------------------------------------------
% --- SPACING OVERRIDES ---
% ----------------------------------------------------------

% \onehalfspacing
\selectfont

%fix spacing after floats
% \setlength{\textfloatsep}{8pt}% Remove \textfloatsep


% ----------------------------------------------------------
\begin{document}

\begin{abstract}
Inverse Reinforcement Learning (IRL) is a technique to learn a reward function that best explains a supervisor's behavior.
These reward functions can be used to find policies for new instances of a task.
However, in multi-step tasks, the learned rewards might be delayed and hard to directly optimize.
One approach is to segment the task into shorter sub-tasks and learn local reward functions.
We present \hirlfull (\hirl), a three-phase algorithm that automatically partitions a task into shorter-horizon sub-tasks based on transitions that occur consistently across demonstrations.
\hirl then learns a sequence of local reward functions using Maximimum Entropy Inverse Reinforcement Learning. 
Once these reward functions are learned, \hirl applies Q-learning to compute a policy that maximizes the rewards. 
We evaluate \hirl in a two simulated control tasks, parallel parking and a two-link pendulum, and two physical experiments using the da Vinci surgical robot, tensioning and cutting deformable sheets.
Our results suggest that \textbf{TODO}
\end{abstract} 

\maketitle



\input{1-wafr16-intro.tex}
\input{2-wafr16-relatedwork.tex}
\input{3-wafr16-background.tex}
\input{4-wafr16-segmentation.tex}
\input{5-wafr16-reward.tex}
\input{5-2-wafr16-policy.tex}
\input{6-wafr16-experiments.tex}
\input{7-wafr16-conclusion.tex}

\vspace{0.5em}

{\footnotesize 
\noindent \textbf{Acknowledgements:}
This research was performed at the AUTOLAB at UC Berkeley in
affiliation with the AMP Lab, BAIR, and the CITRIS "People and Robots" (CPAR) Initiative in affiliation with UC Berkeley's Center for Automation and Learning for Medical Robotics (Cal-MR). The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, and by Google, UC Berkeley's Algorithms, Machines, and People Lab, Knut \& Alice Wallenberg Foundation, and by a major equipment grant from Intuitive Surgical and by generous donations from Andy Chou and Susan and Deepak Lim. We thank our colleagues and the anonymous WAFR reviewers who provided valuable feedback and suggestions, in particular, Pieter Abbeel, Anca Dragan, and Roy Fox.}


%\href{http://j.mp/v-tsc}{j.mp/v-tsc}

\bibliographystyle{SageH}
\bibliography{deepP2P,gmm,gmm2,gmm3}

%\appendix
%\input{appendix.tex}

\end{document}