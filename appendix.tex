\section{Appendix}

\subsection{Sequence of Stable Feedback Controllers}\label{sec:appendix1}
The proposed model naturally arises from a system controlled with linear state feedback controllers to the centroids of the $k$ target regions $[\rho_1,...,\rho_k]$. 
In the Transition State model,  $[\rho_1,...,\rho_k]$ are defined as the sublevel sets of multivariate Gaussian distributions.
For each of the Gaussian mixture components, let $[\mu_1,...,\mu_k]$ denote the respective expectations and $[\Sigma_1,...,\Sigma_k]$ denote the respective covariances.
We can show that the Transition State Clustering model naturally follows from of a sequence of stable linear full-state feedback controllers sequentially controlling the system to each $\mu_i$ (up-to some tolerance defined by $\alpha$).

Suppose, we model the agent's trajectory in feature space as a linear dynamical system with a fixed dynamics.
Let $A_r$ model the agent's linear dynamics and $B_r$ model the agent's control matrix:
\[
\mathbf{x}(t+1) = A_r\mathbf{x}(t) + B_r\mathbf{u}(t) + W(t).
\]
For a particular mixture component $i$, the agent applies a linear feedback controller with gain $C_i$, regulating around the target state $\mu_i$.
This can be represented as the following system (by setting $u(t)=-C_i\hat{\mathbf{x}}$):
\[
\hat{\mathbf{x}}(t) = \mathbf{x}(t) - \mu_i.
\]
\[
\hat{\mathbf{x}}(t+1) = (A_r-B_rC_i)\hat{\mathbf{x}}(t)+ W(t).
\]
If this system is stable, it will converge to the state $\hat{\mathbf{x}}(t) = \mathbf{0}$ which is  $\mathbf{x}(t) = \mu_i$ as $t \rightarrow \infty$.
However, since this is a finite time problem, we model a stopping condition, namely, the system is close enough to $\mathbf{0}$.
For some $z_\alpha$ (e.g., in 1 dimension 95\% quantiles are $Z_{5\%} = 1.96$):
\[
\hat{\mathbf{x}}(t)^T \Sigma^{-1}_i \hat{\mathbf{x}}(t) \le z_\alpha.
\]
If the agent's trajectory was modeled as a sequence $1...K$ of such controllers, we would observe the Transition State Clustering model with each $A_i = A_r-B_rC_i$, and the clusters would be an estimate of the $(\mu_i, \Sigma_i)$.

\subsection{Mixture Models And Linear Systems}\label{sec:appendix2}
Using a GMM to detect switches in local linearity is an approximate algorithm that has been applied in a number of prior works~\cite{moldovan2013dirichlet,calinon2014task, khansari2011learning}.
This is akin to a using a Gaussian kernel for kernelized change point detection~\cite{harchaoui2009kernel}.
We provide some intuition on why this model is sensible for our application.

Consider the following dynamical system:
\[
x_{t+1} = f(x_{t}) + w_{t}
\]
where $w_{t}$ is unit-variance i.i.d Gaussian noise $N(0,I)$.
Let us first focus on linear systems.
If $f$ is linear, then the problem of learning $f$ reduces to linear regression:
\[
\arg\min_{A} \sum_{t=1}^{T-1} \|Ax_{t} - x_{t+1}\|.
\]
Alternatively, we can think about this linear regression probabilistically.
Let us first consider the following proposition:

\vspace{0.75em}

\begin{proposition}
Consider the one-step dynamics of a linear system.
Let $x_{t} \sim N(\mu, \Sigma)$, then $\binom{x_{t}}{x_{t+1}}$ is a multivariate Gaussian.
\end{proposition}
\begin{proof}
This follows from the fact that $x_{t+1}$ can be expressed as a linear combination of independent multivariate Gaussian random variables.
\end{proof}

\vspace{0.5em}

Following from this idea, if we let $p$ define a distribution over $x_{t+1}$ and $x_{t}$:
\[
p(x_{t+1},x_{t}) \sim Normal
\]
For multivariate Gaussians the conditional expectation is a linear estimate, and we can see that it is equivalent to the regression above:
\[
\arg\min_{A} \sum_{t=1}^{T-1} \|Ax_{t} - x_{t+1}\| = \mathbf{E}[x_{t+1} \mid x_{t}].
\]

The GMM model allows us to extend this line of reasoning to consider more complicated $f$.
If $f$ is non-linear $p$ will almost certainly not be Gaussian.
However, GMM models can model complex distributions in terms of Gaussian Mixture Components:
\[
p(x_{t+1},x_{t}) \sim GMM(k)
\]
where $k$ denotes the number of mixture components.
The interesting part about this mixture distribution is that locally, it models the dynamics as before.
Conditioned on particular Gaussian component $i$ the conditional expectation is:
\[
\mathbf{E}[x_{t+1} \mid x_{t}, i \in 1...k].
\]
As before, conditional expectations of Gaussian random variables are linear, with some additional weighting $\phi(i \mid x_{t},x_{t+1})$:
\[
\arg\min_{A_i} \sum_{t=1}^{T-1} \phi(i \mid x_{t},x_{t+1}) \cdot \|A_ix_{t} - x_{t+1}\|.
\]
Every tuple $(x_{t+1},x_{t})$ probability $\phi(i \mid x_{t},x_{t+1})$ of belonging to each $i$th component, and this can be thought of as a likelihood of belonging to a given locally linear model.

\subsection{Alternative Approaches}
We consider the following alternative approaches to compare against \hirl. 

\subsubsection{RL}
This approach considers no segmentation and no history. It directly applies forward RL to the apparent state-space and uses a distance-to-goal reward function.

\subsubsection{Sliding Window}
This approach considers no segmentation but includes a sliding window of $k$ previous states in the state-space. It directly applies forward RL to the augmented state-space and uses a distance-to-goal reward function.

\subsubsection{IRL}
This approach uses MaxEnt-IRL to learn a reward function without segmentation and requires $N=5$ demonstrations. We apply forward RL to the learned reward function. 

\subsubsection{Endpoint Model}
This is a simplified approach to construct rewards using the learned $G$.
Let $\{\mu_1,...,\mu_k\}$ be the set of all of the means of $G$ learned with the algorithm in the previous section.
These means are in the feature space $\mathbb{R}^p$.
Let $\gamma$ denote the current progress of the task, i.e., the previously achieved goal  + 1. 
We can define a reward function as follows:
\[
R(s,a) = -\|f(s,a) - \mu_{\gamma}\|_2^2 
\]

\subsection{Counter-examples}
We constructed two scenarios, a Maze and a Pinball domain, in which \hirl actually performs worse than the alternatives. In the Maze domain, there is a 2D grid with a single unique solution path to a goal state.
In this problem, the segments found by \hirl provide no additional information compared to IRL.
In the Pinball domain, there is a ball on a table with obstacles that is moved by tilting the table.
The ball has elastic collisions with the obstacles and has noisy dynamics.
In this domain, we find that \hirl tends to over-segment this problem since every collision results in another linear regime. 

\begin{table}[ht]
\scriptsize
    \centering
    \begin{tabular}{|r|r|r|r|r|}
    \hline
         %&  \multicolumn{2}{c|}{2D-MP-2}& %\multicolumn{2}{c|}{Two-Rooms} & 
         &\multicolumn{2}{c|}{Maze}& %\multicolumn{2}{c|}{RC(FO)}
         %& \multicolumn{2}{c|}{RC(PO)}& %\multicolumn{2}{c|}{Acrobot} & 
         \multicolumn{2}{c|}{Pinball}\\
    \hline
       & Max & AUC & Max & AUC\\
    \hline
        RL &  $\mathbf{0.960}$ & $2.575$& $0.481$ &$6.941$\\
    %\hline
    %    Sliding & $0.924$ & $-0.855$ & $0.242$ &$1.042$\\
    \hline
        IRL & $0.914$ & $\mathbf{3.575}$ & $0.424$ &$\mathbf{10.904}$\\
    \hline
        TSC+Endpoints & $0.944$ & $-0.448$&  $\mathbf{0.793}$ & $9.315$\\
    \hline
        \hirl & $0.924$ & $1.448$ & $0.722$ &$8.331$\\
    \hline
   % \hline
%        Perfect+Endpoints & - &-& $0.884$ & $34.236$\\
%    \hline
%        Perfect+\hirl & - & -& $0.934$ & $57.129$\\
%    \hline
    \end{tabular}
    \caption{This table summarizes the convergence rate and max reward attained by a Q-learning agent using different reward and state-space representations on domains that were constructed to be counter-examples. \hirl does not perform as well in domains where there is a single path to the goal state. In this case, IRL finds the path and the additional states added by \hirl can actually impede convergence. }
    \label{tab:my_label}
\end{table}