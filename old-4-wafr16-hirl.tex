




%===============================================================















\iffalse
Suppose we are given a set of trajectories (demonstrations) $\mathcal{D}=\{d_1,...,d_N\}$ that are said to be generated from $\pi^*$ but w.r.t an unknown reward function.
Inverse Reinforcement Learning (IRL) studies the problem of inferring $R$ from $\mathcal{D}$.
IRL is a form of learning from demonstrations, but crucially differs from policy learning approaches (see ~\cite{schaal1997learning}) as it learns a reward function and not a policy.
The motivation is that this reward function is more likely to transfer to a perturbed version of the task $\mathcal{M}$, e.g., with different dynamics $P'(\cdot,\cdot)$, than a learned policy.



The objective is to infer $R(\cdot,\cdot)$ given the demonstrations~\cite{DBLP:conf/icml/NgHR99,DBLP:conf/aaai/ZiebartMBD08, coates2008learning}.
However, typically, it is impractical to observe enough data to learn the function $R(\cdot,\cdot)$ exactly.
Therefore, we often formulate the problem with parametrized reward functions:
\[
R_{\theta}(s,a) = \phi(f(s,a);\theta).
\]
where $f(s,a)$ is a feature vector in $\mathbb{R}^p$, $\theta$ is a parameter vector from some parameter space $\Theta \subseteq \mathbb{R}^q$, and $\phi$ describes the relationship between the two.
For example, we may restrict ourselves to the class of linear functions of the features:
\[
R_{\theta}(s,a) = f(s,a)^T\theta.
\]
We will use the notation $\pi_{R | \theta}^*$ to denote an optimal policy with respect to the reward function parametrized by $\theta$.

\subsection{Local Rewards}
Existing IRL algorithms infer a single reward function $R_{\theta}$ to describe an observed agent's behavior.
In contrast, consider the following model where the agent's behavior is described in terms of local rewards.
Let $[R_{\theta_1}, R_{\theta_2}, ..., R_{\theta_k}]$ be a sequence of reward functions. 
Assume that each $\theta_i$ defines a reward that is not policy-equivalent~\cite{DBLP:conf/icml/NgHR99} to any other $\theta_j$. 
The agent starts by applying the policy $\pi_{R | \theta_1}^*$ (and accumulating rewards w.r.t $R_{\theta_1}$) until it reaches a ``transition state'', which is a state $s$ in some region $\rho_1$. 
Then, the agent switches to policy $\pi_{R | \theta_2}^*$ (and accumulating rewards w.r.t $R_{\theta_2}$) and continues until $\rho_2$ is reached, and so on.
This behavior continues until $\rho_k$ is reached at which point the episode terminates.

\begin{definition}[Local Reward Model]\label{def:lreward}
For a state-space $S$, action-space $A$, and system dynamics $P$, a sequential task is defined by a sequence of tuples of non-equivalent reward functions $R_{\theta_i}: S \times A \mapsto \mathbb{R}$ and transition regions $\rho_i$:
\[
\mathbf{R}_{seq} = [ (R_{\theta_1}, \rho_1) , (R_{\theta_2}, \rho_2), ..., (R_{\theta_k}, \rho_k) ]
\]
\end{definition}

It is important to note that the class of tasks that can be represented with local rewards is strictly larger than the class of tasks with a single deterministic reward function both in the finite-horizon and infinite-horizon case (i.e., $\rho_k = \emptyset$). A corollary to this definition is notion of a locally-optimal agent:

\begin{definition}[Locally Optimal Agent]\label{def:lagent}
Given a task defined by a local reward sequence $\mathbf{R}_{seq}$, an agent is said to be locally optimal w.r.t $\mathbf{R}_{seq}$ if for each reward function $R_{\theta_i}$ it applies a policy $\pi_{R | \theta_i}^*$ which is optimal w.r.t $R_{\theta_i}$ in the infinite horizon (i.e., ignoring the transition upon reaching $\rho_i$). 
\end{definition}

\subsection{\hirlfull Problem}
Based on the Definition \ref{def:lreward} and \ref{def:lagent}, the \hirlfull problem is to infer $\mathbf{R}_{seq}$ given observations of a locally optimal agent.

\begin{definition}[\hirlfull]
Given $\mathcal{D}=\{d_1,...,d_N\}$, a set of trajectories of state-action tuples from a locally-optimal agent, and a system dynamics model $P$, infer the reward sequence $\mathbf{R}_{seq}$.
\end{definition}
\fi
