\section{Introduction}
A primary objective of robot learning from demonstrations is to learn policies that generalize beyond the provided examples and are robust to perturbations in initial conditions, the environment, and sensing noise~\cite{argall2009survey}.
A popular approach is Inverse Reinforcement Learning (IRL), where the demonstrator is optimizing 
an unknown reward function and the problem is to infer this function from a set of demonstrations~\cite{DBLP:conf/nips/KolterAN07, coates2008learning, abbeel2004apprenticeship}.
Once a reward is learned, given novel instances of a task, a policy can be computed by optimizing for this reward function using an approach like Reinforcement Learning (RL)~\cite{abbeel2004apprenticeship}.

In classical IRL, tasks are modeled as MDPs with fixed reward functions that maps state and action tuples to scalar values. 
This model is limited in the way that it can represent \emph{sequential tasks}, tasks where a robot must reach a sequence of intermediate state-space goals.
Ignoring the sequential structure of task may have severe consequences during policy learning.
First, the inferred reward function may be \emph{delayed}
and reflect a quantity observed after all of the goals are reached -- making it very difficult to optimize directly, e.g., learning to minimize scar volume in robot surgical suturing rather than uniformity of each stitch.
Furthermore, it is also possible that the sequence is such that there does not exist a single stationary policy (a time-invariant map between states and actions) that achieves all of the goals in sequence, e.g., a figure 8 trajectory in the x,y plane.

For such tasks, it is necessary to divide the task into segments and local reward functions.
In existing work on multi-step IRL, this sequential structure is defined manually~\cite{DBLP:conf/nips/KolterAN07}.
We explore automatically learning sequential structure as well as assigning local reward functions to segments.
The combined problem is nontrivial because solving $k$ independent problems neglects any shared structure in the value function during the policy learning phase (e.g., a common failure state).
However, jointly optimizing over the segmented problem inherently introduces a dependence on history, namely, any policy must complete step $i$ before step $i+1$.
Modeling an arbitrary dependence on history potentially leads to an exponential overhead of additional states, and thus, the key algorithmic challenge is to formulate a segmentation and IRL problem where this dependence can be represented efficiently.

\hirlfull (\hirl) is a three-phase algorithm for learning a policy from a set of demonstrations of a sequential task.
\hirl is based on new formalism for sequential tasks representing them as a sequence of reward functions $\mathbf{R}_{seq}=[R_1,...,R_k]$ and transition regions (subsets of the state-space) $G = [\rho_1, ...,\rho_k]$ such that $R_1$ is the reward function until $\rho_1$ is reached, after which $R_2$ becomes the reward and so on.
\hirl assumes that demonstrations have locally linear dynamics w.r.t a provided feature space, are locally optimal (as in IRL), and all demonstrations reach every $\rho \in G$ in the same sequence.
In the first phase of the algorithm, we infer the transition regions using a kernelized variant of an algorithm proposed in our prior work~\cite{krishnan2015tsc,murali2016}.
In the second phase, we use the inferred transition regions to segment the set of demonstrations, and apply IRL locally to each segment to construct the sequence of reward functions $\mathbf{R}_{seq}$.
Finally in the third phase, \hirl  computes a policy using an RL algorithm (Q-Learning) over an augmented state-space that indicates the sequence of previously reached reward transition regions, and we show that this augmentation has an additional space complexity independent of the state-space and linear in the number of rewards.

Our contributions are:
\begin{itemize}
\item We propose a new formalism for sequential tasks, where a task is modeled by an MDP with a sequence of reward functions, and an algorithm called \hirl to learn policies for such tasks.
\item The first phase of \hirl (sequence learning) is an extension of the Transition State Clustering model proposed in our prior work that relaxes the local-linearity assumption using kernelization.
\item \hirl enforces sequential dependencies in policies by a novel state-space augmentation with an indicator of the previously completed segments, which can be efficiently stored and computed based on the first phase. 
\item Experiments suggest that \hirl can generalize from the demonstrations to unseen examples and is robust to noise in the environment and initial conditions. 




%\todo{Add numbers}
