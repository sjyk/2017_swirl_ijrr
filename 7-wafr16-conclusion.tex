\section{Discussion and Future Work}
\textbf{SK. TODO revise}
\hirl is a three-phase algorithm that first segments a task, learns local rewards, and then learns a policy.
Experimental results suggest that sequential segmentation can indeed improve convergence in RL problems with delayed rewards.
Results suggest that \hirl is robust to perturbations in initial conditions, the environment, and sensing noise.
This paper formalizes the interaction and composability of the three phases (sequence, reward, and policy learning). In future work, we will explore extensions to each of the phases and quantify the degree of generalization.
We will explore how the Q-Learning step could be replaced with Guided Policy Search, Policy Gradients, and optimal control.
We will modify the segmentation algorithm to incorporate more complex transition conditions and allow for sub-optimal demonstrations.
We will explore more robotic tasks including suturing, surgical knot tying, and assembly.
Another avenue for future work is modeling complex tasks as hierarchies of MDPs.

% Another avenue for future work is modeling complex tasks as hierarchies of MDPs, namely, tasks composed of multiple MDPs that switch upon certain states and the switching dynamics can be modeled as another MDP. 