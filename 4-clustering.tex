%\documentclass[0-main.tex]{subfiles}
%\begin{document}

\section{\tshfull}\label{sec:algo}
This section describes the \tsh algorithm, which has four steps: (1) featurization, (2) transition identification, (3) transition correspondence, and (4) vectorization.

\subsection{Step 1. Featurization}
The first step is to apply $f$ to every state-action pair in the demonstrations.
Every $d_i$ in $D$ is a sequence of $T$ state-action pairs:
\[
[(s_1,a_1),...,(s_T,a_T)]
\]
For each state-action pair, we apply the featurization $ f(s_1,a_1),...,f(s_T,a_T)$.
This gives us a $T$-step trajectory in the feature space $\mathbb{R}^p$ which we denote as $\mathbf{x}(t)$.
In this work, we assume that the featurization is given to us, and hope to explore using techniques such as Deep Learning in the future.

\subsection{Step 2. Identification}
This subsection is largely inspired from a model proposed in prior work~\cite{krishnan2015tsc}.
Each demonstration, $\mathbf{x}(t)$, is a trajectory in $\mathbb{R}^p$.
Suppose there was only one dynamical mode, then following from the Gaussian process noise assumption, this would be a linear regression problem:
\[
\arg\min_A \|A X_t - X_{t+1}\|
\]
where $X_t$ is a matrix where each column vector is $\mathbf{x}(t)$.
To extend this to set of $A$ matrices, Moldovan et al.~\cite{moldovan2013dirichlet} proves that fitting a Jointly Gaussian model to $w(t) = \binom{\mathbf{x}(t+1)}{\mathbf{x}(t)}$ is equivalent to Bayesian Linear Regression.
The intuition is that Gaussian random variables are stable under affine transformations, and applying a Mixture of Gaussians model (GMM) essentially applies locally linear regression.
If we map each $w(t)$ to its most likely mixture component, then we can find the co-linear states.
In typical GMM formulations, we have to select the number of mixture components $m$ before hand.
However, we can apply new results in Bayesian non-parameteric statistics and jointly solve for the component locations and the number of components with an algorithm called DP-GMM~\cite{kulis2011revisiting} with a soft prior over the number of clusters\footnote{We use the default settings in \url{https://pypi.python.org/pypi/dpcluster}}.

The identification procedure is summarized in Algorithm \ref{alg:tsh1}.
We construct a data matrix $W$ which is the set of $w(t)$ over all demonstrations and times.
We apply DP-GMM to $W$ and map each $w(t)$ to a most likely mixture component, and thus for each demonstration $i$ we get $w_{index}^{(i)}(t)$ which gives us the cluster index at time $t$.
Then, we identify all of the times $t$ such that $w_{index}^{(i)}(t) \ne w_{index}^{(i)}(t+1)$, and the final result is a set $\Theta$ of tuples of demonstration id $i$ and time $t$.

\subsection{Kernel Dimensionality Reduction}\label{sec:kern}
Transition identification makes an assumption about local-linearity.
However, we can relax this assumption by embedding the trajectories in a non-linear kernel space and identifying transitions in the embedded space.
Let $W = \{w_i\}$ be an indexed set of all $w(t)$ over all demonstration.
Let $\mathbf{\kappa}(w_0,w_1)$ define a kernel function over the set $W$.
For example, if $\mathbf{\kappa}$ is the radial basis function (RBF), then:
$ \mathbf{\kappa}(w_0,w_1) = e^{\frac{-\|w_0-w_1\|_2^2}{2\sigma}}$.
$\mathbf{\kappa}$ naturally defines a matrix $M$ where:
\[
M_{ij} = \mathbf{\kappa}(w_i,w_j) 
\]
The top $p'$ eigenvalues define a new embedded feature vector for each $w$ in $\mathbb{R}^{p'}$.
In this embedded space, we can apply our transition identification procedure.
This procedure allows us to model some types of non-linearities and states with different scaling properties.
In our experiments, we use an RBF kernel function in the cases where the feature-space dynamics were non-linear. 

\begin{algorithm}[t]
\small
\DontPrintSemicolon
\caption{Transition Identification \label{alg:tsh1}}
\KwData{Set of demonstrations:$\mathcal{D}$}

$W \leftarrow \emptyset$

\ForEach{$d_i \in \mathcal{D}$}{
   \ForEach{$t \in 0,1,...,T_i$}{
        $w(t) = \binom{\mathbf{x}(t+1)}{\mathbf{x}(t)}$
        
        $W = W \cup w(t)$
   }
}
$\{w_{index}^{(1)}(t),...,w_{index}^{(N)}(t)\} \leftarrow$ \texttt{DP-GMM(W)}
\vspace{0.5em}

$\Theta \leftarrow \emptyset$

\ForEach{$d_i \in \mathcal{D}$}{
   \ForEach{$t \in 0,1,...,T_i$}{
        \If{$w_{index}^{(i)}(t) \ne w_{index}^{(i)}(t+1)$}{
        $\Theta = \theta \cup (i,t)$
        }
   }
}

\KwResult{The set of transitions $\Theta$}
\end{algorithm}

\subsection{Step 3. Correspondence}
Across all demonstrations, the set of transition states induces a density over the feature-space and time.
Intuitively, when an agent enters certain regions of the state-space at certain times, there is a propensity to switch.
We are interested in aggregating nearby (spatially and temporally) transition states together.
We model this density as a Gaussian Mixture Model with $k$ mixture components $\{m_1,...,m_k\}$.
As before, we learn this with DP-GMM in the feature space to find these clusters.

Each of the mixture components is a multivariate Gaussian distribution with some mean and covariance.
Individually, each of the mixture components is a Gaussian distribution, and defines an $\alpha$-quantile region of the feature space and a time interval:
\[
G = \{\rho_1, \rho_2,...,\rho_m\}
\]
The overall procedure is summarized in Algorithm \ref{alg:tsh2}.

\begin{algorithm}[t]
\small
\DontPrintSemicolon
\caption{Transition State Clustering \label{alg:tsh2}}
\KwData{Set of transitions:$\Theta$}

$Y \leftarrow \emptyset$

\ForEach{$(i,t) \in \Theta$}{
   $Y \leftarrow Y \cup x^{(i)}(t)$
}
$G = \{\rho_1, \rho_2,...,\rho_m\}\leftarrow$ \texttt{DP-GMM(Y)}

\KwResult{The set of transition state clusters $G$}
\end{algorithm}

\subsection{Step 4. Vectorization}
Next, we show how to turn $G$ into a hash function $C_k$.
Suppose we have a trajectory of states $\mathbf{x}(t)$ from 0 to $T$.
This trajectory defines $T-1$ history vectors $H_2,...,H_T$ of all preceding states until time $T$.
For each $H_t$, we have our encoding function:
\[
e(H_t) = \sum_{i=1}^t C_k(s_i)
\]
Let $k$ be the number of GMM mixture components found by the procedure in the previous subsection.
For each $s_i$, we define an $k$ dimensional binary vector where component $j$ is 1: if $i$ is contained in the time interval $\tau_j$, the state $s_i$ is contained in $\rho_j$, and all previous components $1...j-1$ are 1. 
Thus, $e(H_t)$ the number of time-steps spent in each of the transition states conditioned on the event that all previous transition states are visited.
Algorithm \ref{alg:tsh3} summarizes this process.

\begin{algorithm}[t]
\small
\DontPrintSemicolon
\caption{Transition State Hashing \label{alg:tsh3}}
\KwData{Set of transition state clusters:$G$}
\KwData{Set of history states to hash:$H_t$}

$E \leftarrow [0,...,0]$

\ForEach{$(x,t) \in H_t$}{
   \If{$(x,t) \in$  \texttt{conf}(G)}{
      $i \leftarrow$ \texttt{find}(G,(x,t))
      
      $E[i] \leftarrow E[i] + 1$
   }
}

\KwResult{The hashed value of the history states $E$}
\end{algorithm}

%\end{document}