 \documentclass[letterpaper, 10 pt, conference]{ieeeconf}
 %DRAFT MODE preserves formatting otherwise; %comment out
% \usepackage[draft]{graphics}

\let\labelindent\relax
\input{preamble}

% Macro for Algorithm Name 
\newcommand{\hirl}{HIRL\xspace}
\newcommand{\HIRL}{HIRL\xspace}
\newcommand{\hirlfull}{Hierarchical Inverse Reinforcement Learning\xspace}
\newcommand{\tsh}{TSH\xspace}
\newcommand{\TSH}{TSH\xspace}
\newcommand{\tshfull}{Transition State Hashing\xspace}
\newcommand{\tsco}{TSC\xspace}
\newcommand{\sys}{\textsf{TSC+VIS}\xspace}

\newcommand{\tsce}{\texttt{TSC-Endpoint}\xspace}
\newcommand{\tsci}{\texttt{TSC-IRL}\xspace}

\newboolean{include-notes}
\setboolean{include-notes}{true}
\newcommand{\fp}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{FP: #1}}}{}}
%===============================================================

\title{\LARGE \bf
HIRL: Hierarchical Inverse Reinforcement Learning \\
for Long-Horizon Tasks with Delayed Rewards
}
%Pixels to Primitives: Learning Sub-Task Level Semantic Segmentation \\
%of Multi-Step Task Trajectories from Video with %Deep Learning 

% Visual Transition State Clustering: \del{Using} Deep Learning with \del{to Featurize} Multi-Modal Trajectories For Unsupervised Sub-Task Level Semantic Segmentation \\

% Visual Transition State Clustering: Unsupervised Segmentation of Multi-Modal Trajectories with Deep Learning based Video Features\\

\author{%
Sanjay Krishnan, 
Animesh Garg, 
Richard Liaw,
%Simon E. Zimmerman,\\
Lauren Miller,
Florian T. Pokorny,
% Pieter Abbeel, 
Ken Goldberg%, %{\footnotesize \textcolor{blue}{[v0.5, \today\,\currenttime]}} 
%\thanks{\hrule \vspace{5pt} * These authors contributed equally to the paper}%
\thanks{EECS \& IEOR, University of California, Berkeley CA USA; \texttt{\{sanjaykrishnan, animesh.garg,rliaw, ftpokorny, laurenm, goldberg\}@berkeley.edu}}%
\thanks{This is an extended abstract for arxiv HIRL}%
% \thanks{$^{1}$EECS, University of California, Berkeley; {\{sanjaykrishnan, adithya\_murali\}@berkeley.edu}}%
% \thanks{$^{2}$IEOR and EECS, University of California, Berkeley; {\{animesh.garg, goldberg\}@berkeley.edu}}%
}

\IEEEoverridecommandlockouts %to enable thanks to appear

\begin{document}

%\setlength{\belowdisplayskip}{0.8pt} \setlength{\belowdisplayshortskip}{1pt}
%\setlength{\abovedisplayskip}{0.8pt} \setlength{\abovedisplayshortskip}{1pt}
%\setlength{\belowcaptionskip}{-10pt}
%\selectfont

\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{property}{Property}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\maketitle

%\fontsize{10pt}{12.5pt}
%\selectfont

\section{Introduction}
It is well known that efficacy of Reinforcement Learning (RL) algorithms is highly dependent on how reward functions are specified~\cite{DBLP:conf/icml/NgHR99}.
However, it is often the case that complex tasks with delayed rewards can be segmented into a sequence of sub-tasks with shorter horizons.
Reward functions that capture this structure can lead to more efficient policy search.
We propose a framework called \hirlfull (\hirl), which is a model for learning sub-task structure from demonstrations.
\hirl decomposes the task into sub-tasks based on transitions that are consistent across demonstrations. These transitions are defined as changes in local linearity w.r.t to a kernel function~\cite{krishnan2015tsc}. 
Then, \hirl uses the inferred structure to learn reward functions local to the sub-tasks but also handle any global dependencies such as sequentiality.


The problem of learning skill hierarchies from demonstration without pre-defined motion primitives has been studied before~\cite{niekum2012learning, calinon2014skills,konidaris2009efficient}.
However, to the best of our knowledge such techniques have not been applied to learning hierarchies of rewards (e.g. ~\cite{DBLP:conf/nips/KolterAN07} uses a pre-defined hierarchy in IRL). 
 Rewards are often argued to be more transferable, concise, and easier to interpret~\cite{DBLP:conf/icml/NgHR99}.
Similarly, decomposing MDPs in terms of sub-tasks with local rewards rather than in terms of composite actions may generalize better.

Preliminary results in this paper evaluate HIRL on a simulated parallel parking task to highlight the effects of segmentation on convergence, and robustness to noise. 
In the parallel parking task, we find that rewards constructed with \hirl converge to a policy with an 80\% success rate in 32\% fewer time-steps than those constructed with Maximum Entropy Inverse RL (MaxEnt IRL), and with partial state observation, the policies learned with IRL fail to achieve this accuracy while \hirl still converges.
We further find that that the rewards learned with \hirl are robust to environment noise where they can tolerate 1 stdev. of random perturbation in the poses in the environment obstacles while maintaining roughly the same convergence rate.
We find that \hirl rewards can converge up-to $6\times$ faster than rewards constructed with IRL.
\cite{krishnan2016hirl} provides more detailed evaluations of \hirl on several standard RL benchmarks: Parallel Parking with noisy dynamics, Two-Link Pendulum, 2D Noisy Motion Planning, and a Pinball environment.

\section{HIRL: Model \& Algorithm}
We are given an MDP $\mathcal{M}$ with a known, but difficult to optimize, ``true'' reward function $R_{true}$ that is a binary indicator of success in some robotic task.
Let $D$ be a set of demonstrations $\{d_1,...,d_N\}$ sampled from executions of the optimal policy $\pi^*$ with respect to $R_{true}$. Each demo $d_i$ can be mapped from $x_t$ in $\mathbb{R}^p$ using $f:S\times A \mapsto \mathbb{R}^p$
% We assume that we are given featurization function $f:S\times A \mapsto \mathbb{R}^p$, and with this function, a demonstration is also a trajectory denoted by $x_t$ in $\mathbb{R}^p$.
The goal of \hirl is to construct a reward function $R_{seq}$ with shorter-term rewards whose optimal policy $\pi^\dagger$ approximates the performance of $\pi^*$.

\begin{figure}[t]
\centering
 \includegraphics[width=\columnwidth]{exp/rc-car-segmentation.png}
 \caption{This plot illustrates the 5 demonstration trajectories for the parallel parking task, and the sub-goals learned by \hirl. %Section \ref{exp:pp} describes the details of the experimental setup and the task. \label{exp:rcsegmentation}
 }
\end{figure}

\begin{figure}[t]
\centering
 \includegraphics[width=\columnwidth]{figures/rc-teaser.png}
 \caption{ Comparison of different reward construction techniques: RL (baseline), IRL (from 5 demonstrations), HIRL (from 5 demonstrations).
 The figure shows the convergence of a Q-learning agent with the respective rewards on a parallel parking task with noisy dynamics with full state observations (position, orientation, and velocity) and partial observation (only position and orientation); defining success as the probability that the car successfully parks.
 \label{exp:rcsegmentation-res}}
\end{figure}

We model $R_{true}$ as a sequence of subgoals $\rho \subseteq \mathbb{R}^p$.
And a a task is defined as an \emph{a priori} unknown sequence of sub-goals:
\[
G = [\rho_1,...,\rho_k]
\]
A task is successful, i.e., $R_{true}=1$, when all of the $\rho_i \in G$ are reached in sequence.

\hirl proposes an algorithm to learn $G$ in the case when the regions $\rho_i$ correspond to changes in local linearity, and following from $G$, $R_{seq}$ can be represented as a sequence of local rewards $[R^{(1)}_{seq},...,R^{(k)}_{seq}]$. 
The local reward sequence will serve to guide the agent to each of the $\rho_i$ more efficiently than the sparse true reward $R_{true}$.

\begin{figure}[t]
\centering
 \includegraphics[width=\columnwidth]{exp/rc-car-segmentation-2ab.png}
 \caption{ [A] We vary the number of demonstrations provided to the different techniques and measure the success probability of the policies learned. \hirl has a higher success rate than the alternatives for a small number of demonstrations. [B] We find that policies learned under \hirl rewards a more robust than the alternatives since they leverage segments as ``checkpoints''.  \label{exp:rcsegmentation-res2}}
\end{figure}

\subsection{Locally Linear Sub-goals}
Consider the agent's trajectory in $\mathbb{R}^p$ as a dynamical system,
\[
x_{t+1} = \mathcal{T}(x_{t}) + w_{t},
\]
with i.i.d unit variance Gaussian process noise.
We model sub-tasks as locally-linear, that is, that the system $\mathcal{T}$ can be decomposed into a set of state-dependent linear systems:
\[
x_{t+1} = A_{i}\mathbf{x}_t + w_{t} \text{ : } A_i \in \{A_1,...,A_m\}.
\]
\emph{Transitions} are defined as times where $A_{t} \ne A_{t+1}$.
Thus, each transition will have an associated feature value $\mathbf{x}_{t}$ called a transition state.
The key insight from our prior work~\cite{krishnan2015tsc} is that the transition states have a meaningful spatial structure.
For example, we model these $\mathbf{x}_{t}$ as generated from a Gaussian Mixture Model (GMM) over the feature space $\mathbb{R}^p$. We interpret this mixture model as defining sub-goals for the task.
If there $k$ mixture components for the distribution $\{m_1,...,m_k\}$, the quantile of each component distribution will define sequence of regions $[\rho_1,...,\rho_k]$ over the feature space (i.e., its sublevel set bounded by $z_\alpha$ and ordered by time), and can equivalently be thought of as $\rho_i = (\mu_i,\Sigma_i)$.
We interpret the learned $G = [\rho_1,...,\rho_k]$ as the sub-goals reached by the expert demonstrations.
We detail the procedure of learning these goals automatically in ~\cite{krishnan2015tsc}.

\subsection{State-Space Augmentation}
Reaching transition states are associated with attaining rewards in the task, and when all of the transition states in $G$ are reached in sequence the agent is successful. 
Attaining a reward at goal $\rho_i$ depends on knowing that the reward at goal $\rho_{i-1}$ was attained.

Given a finite-horizon MDP $\mathcal{M}$, we can define an MDP $\mathcal{M}_H$ that captures this dependence.
Let $\mathcal{H}$ denote set of all dynamically feasible sequences of length $\le T$ comprised of the elements of $S$.
Therefore, for an agent at any time $t$, there is a sequence of previously visited states $H_t \in \mathcal{H}$.
The MDP $\mathcal{M}_H$ is defined as:
\[
\mathcal{M}_H = \langle S \times \mathcal{H},A,P'(\cdot,\cdot), R_\theta(\cdot,\cdot),T \rangle.
\]
For this MDP, $P'$ not only defines the transitions from the current state $s \mapsto s'$, but also increments the history sequence $H_{t+1} = H_{t} \sqcup s$.
Accordingly, the parametrized reward function $R_\theta$ is defined over $S$, $A$, and $H_{t+1}$.

By modeling assumption, we know that a sufficient statistic for task success is knowing that all of the transition states $G$ were reached.
We can use this fact to concisely encode the history of the agent $H_t \in \mathcal{H}$ in terms of transition states previously which is a $k$ dimensional vector $\{0,1\}^k$.


\subsection{Reward Learning and Policy Evaluation}
With this model and the state-space augmentation, we can apply standard techniques for inverse and ``forward" RL.

\vspace{0.5em} \noindent \textbf{Inverse Reinforcement Learning: } We can apply standard techniques for IRL over the augmented state-space $S \times  \{0,1\}^k$. Suppose, we are considering linear functions of features of state-action tuples $R_{\theta}(s,a) = f(s,a)^T\theta$, we can apply the same reasoning to state-action-segment tuples $R_{\theta}(s,a) = \binom{f(s,a)^T}{v}\theta$, where $v \in \{0,1\}^k$ and indicates the sub-goal progress. Then, conditioned on each possible $v$ (i.e., the current task progress), we can find the local reward sequence $[R^{(1)}_{seq},...,R^{(k)}_{seq}]$. 
In principle, we can apply any IRL technique, and in this work, we apply the widely used Maximum Entropy IRL~\cite{DBLP:conf/aaai/ZiebartMBD08}.

\vspace{0.5em} \noindent \textbf{Rewards to Policies: }
We can use any policy learning method for a given reward function. This paper uses Q-learning, however we note that could also use a framework like Guided Policy Search ~\cite{DBLP:journals/corr/LevineFDA15} or even an optimal control framework such as iLQR. 

% In principle, we can apply many different policy learning techniques to learn a policy given the reward function over the augmented state-space $S \times  \{0,1\}^k$. In this paper, we use Q-learning to address the policy learning problem, and evaluate the claim of whether \hirl rewards lead to faster convergence. However, one could also apply these learned rewards in a framework like Guided Policy Search~\cite{DBLP:journals/corr/LevineFDA15}, or even in an optimal control framework like iLQR~\cite{li2004iterative}.

% \section{Experiments}\label{sec:exp}


% \input{1-intro.tex}
% \input{2-relatedwork.tex}
% \input{3-model.tex}
%\subfile{3-problemsetup.tex}
% \input{4-segmentation.tex}
% \input{5-embedding.tex}
% \input{5-reward-design.tex}
% \input{6-results.tex}
% \input{7-conclusion.tex}

% {\footnotesize 
% \noindent \textbf{Acknowledgements:}
% We would like to thank Anca Dragan, Stuart Russell, and Pieter Abbeel for discussions about this work.
% We would also like to thank Daniel Seita, Jeff Mahler, and Michael Laskey for their feedback on early drafts.
% This research was performed with UC Berkeley's Automation Sciences Lab under the UC Berkeley Center for Information Technology in the Interest of Society (CITRIS)``People and Robots'' Initiative http://robotics.citris-uc.org, and 
% UC Berkeley's Algorithms, Machines, and People Lab.}

%\href{http://j.mp/v-tsc}{j.mp/v-tsc}

% \normalsize \selectfont
\bibliographystyle{IEEEtranS}
\bibliography{deepP2P}

% \appendix
% \input{appendix.tex}

\end{document}