
\section{\hirl: Reward Learning}\label{sec:reward}
After the sequence learning phase, each demonstration is partitioned into $k$ segments.
The reward learning phase uses the learned $[\rho_1,...,\rho_k]$ to construct the local rewards $[R_1,...,R_k]$ for the task.
Each $R_i$ is a quadratic cost parametrized by a positive semi-definite matrix $Q$.
The Algorithm is summarized in Phase\,\ref{alg:tsh2}.


\subsection{Maximum Entropy Inverse Reinforcement Learning}
To fit the local rewards, we apply Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL)~\cite{DBLP:conf/aaai/ZiebartMBD08}. 
The goal of MaxEnt-IRL is to find a reward function such that an optimal policy w.r.t that reward function is close to the
expert demonstration.
In the MaxEnt-IRL model, ``close'' is defined as matching the first moment of the expert feature distribution:
\[
\gamma_{expert} = \frac{1}{Z} \sum_{d \in D} \sum_{i=1}^N x_i,
\]
where $Z$ is an appropriate normalization constant (total number of states in all demonstrations).
MaxEnt-IRL uses the following linear parametrized representation:
\[
R(s,a) = x^T \theta,
\]
where $x$ is a feature vector.
The agent is modeled as noisly optimal, where it takes actions from a policy $\pi$:
\[
\pi(a \mid s, \theta) \propto \exp\{A_\theta(s,a)\}.
\]
$A_\theta$ is the advantage function (Q function minus the Value function) for the reward parameterized by $\theta$.
The objective to to maximize the log likelihood that the demonstration trajectories were generated by  $\theta$.

Under the exponential distribution model, it can be shown that the gradient for this likelihood optimization is:
\[
\frac{\partial L}{d \theta} = \gamma_{expert} - \gamma_{\theta},
\]
where $\gamma_{\theta}$ is the first moment of the feature distribution of an optimal policy under $\theta$.

\hirl applies MaxEnt-IRL to each segment of the task but with a small modification to learn quadratic rewards instead of linear ones. However, this is a simple re-parametrization of the problem without loss of generality.
Let $\mu_i$ be the centroid of the next transition region.
We want to learn a reward function of the form:
\[
R_i(x) = -(x-\mu_i)^T Q (x-\mu_i).
\]
for a positive semi-definite $Q$ (negated since this is a negative quadratic cost).
With some re-parametrization, this reward function can be written as:
\[
R_i(x) = -\sum_{j=1}^d \sum_{l=1}^d q_{ij} x[j] x[l].
\]
which is linear in the feature-space $y = x[j] x[l]$:
\[
R_i(x) = \theta^T y.
\]

\subsection{Inference Settings}
In MaxEnt-IRL gradient can be estimated reliably in two cases, discrete and linear-gaussian systems, since it requires an efficient forward search of the policy given a particular reward parametrized by $\theta$.
In both these cases, we have to estimate the system dynamics within each segment.

\subsubsection{Discrete}
Consider the case when the state-space is discrete (with cardinality $N$) and the action-space is discrete. 
To estimate the dynamics, we construct an $N \times N$ matrix of zeros for each action and each of the components 
of this matrix corresponds to the transition probability of a pair of states.
For each, $(s,a,s')$ observation in the segment, we increment (+1) the appropriate element of the matrix.
At the end, we normalize the elements to sum to one across the set of actions.
An additional optimization could be to add smoothing to this estimate (i.e., initialize the matrix with some non-zero constant value), we found that this was not necessary on the sparse domains in our experiments.
The result is an estimate for the $P(s' \mid s, a)$.
Given this estimate, $\gamma_{\theta}$ can be efficiently calculated with the forward-backward technique described in~\cite{DBLP:conf/aaai/ZiebartMBD08}.

\subsubsection{Linear}
The discrete model is difficult to scale to continuous state-spaces.
If we discretize, the number of bins required would be exponential in the dimensionality.
However, linear models are another class of dynamics models for which the estimation and inference is tractable.
We can fit local linear models to each of the segments discovered in the previous section:
\[
A_j = \arg\min_{A} \sum_{i=1}^N \sum_{\text{seg j start}}^{\text{seg j end}} \|A x^{(i)}_{t} - x^{(i)}_{t+1}\|
\]
With $A_j$ known, $\gamma_{\theta}$ can be analytically solved with techniques proposed in~\cite{ziebart2012probabilistic}.
\hirl applies MaxEnt-IRL to the sub-sequences of demonstrations between 0 and $\rho_1$, and then from $\rho_1$ to $\rho_2$ and so on.
The result is an estimated local reward function $R_{i}$ modeled as a linear function of states that is associated with each $\rho_i$.

\begin{phase}[t]
\small
\DontPrintSemicolon
\caption{Reward Learning \label{alg:tsh2}}
\KwData{Demonstration $\mathcal{D}$ and sub-goals $[\rho_1,...,\rho_k]$}

Based on the transition states, segment each demonstration $d_i$ into $k$ sub-sequences where the $j^{th}$ is denoted by $d_i[j]$.

If dynamics model is available, apply MaxEnt-IRL to each set of sub-sequences $1...k$.

If the dynamics model is not available compute Equation \ref{localq} for each set of subsequences.

\KwResult{$\mathbf{R}_{seq}$}
\end{phase}

\subsubsection{Model-free: Local Quadratic Rewards}
Sometimes estimating the local dynamics can be unreliable if there isn't sufficient demonstration data.
As a baseline, we also considered a much simpler reward learning approach that just estimates the covariance in each feature.
Interestingly enough, this approach worked reasonably well empirically in many problems.

The role of the reward function is to guide the robot to the next transition region $\rho_i$.
A straight forward thing approach is for each segment $i$, we can define a reward function as follows:
\[
R_i(x) = -\|x - \mu_{i}\|_2^2, 
\]
which is just the Euclidean distance to the centroid.

A problem with using Euclidean distance directly is that it uniformly penalizes disagreement with $\mu$ in all dimensions.
During different stages of a task, some features will likely naturally vary more than others--this is learned through IRL.
To account for this, we derive a reasonable $Q$ that is independent of the dynamics:
\[
Q[j,l] = \Sigma^{-1}_x,
\]
which is the inverse of the covariance matrix of all of the state vectors in the segment:
\begin{equation}
Q[j,l] = (\sum_{t=start}^{end} x x^T)^{-1},
\label{localq}
\end{equation}
which is a $p \times p$ matrix defined as the covariance of all of the states in the segment $i-1$ to $i$.
Intuitively, if a feature has low variance during this segment, deviation in that feature from the desired target it gets penalized. 
This is exactly the Mahalonabis distance to the next transition. 

For example, suppose one of the features $j$ measures the distance to a reference trajectory $u_t$. 
Further, suppose in step one of the task the demonstrator's actions are perfectly correlated with the trajectory ($Q_{i}[j,j]$ is low where variance is in the distance) and in step two the actions are uncorrelated with the reference trajectory ($Q_{i}[j,j]$ is high).
Thus, $Q$ will respectively penalize deviation from $\mu_{i}[j]$ more in step one than in step two.