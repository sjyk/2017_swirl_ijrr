\section*{\hirl: Reward Learning}\label{sec:reward}
After the sequence learning phase, each demonstration is segmented into $k$ segments.
The reward learning phase uses the learned $[\rho_1,...,\rho_k]$ to construct the local rewards $[R_1,...,R_k]$ for the task.
Each $R_i$ is a quadratic cost parametrized by a positive semi-definite matrix $Q$.
The Algorithm is summarized in Phase\,\ref{alg:tsh2}.

\subsection{Reward Shaping with IRL}



\subsubsection{Model-based}
For the model-based approach, we use Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL)~\cite{DBLP:conf/aaai/ZiebartMBD08}. 
The idea is to model every demonstration $d_i$ as a noisy sample from an optimal policy.
In other words, each $d_i$ that is observed is a noisy observation of some hypothetical $d^*$.

Since each $d_i$ is a path through a possibly discrete state and action space, we cannot simply average them to find $d^*$.
Instead, we have to model trajectories that the system is likely to visit. 
This can be formalized with the following probability distribution:
\[
P(d_i | R) \propto \exp \{ \sum_{t=0}^T R(s_t) \}.
\]
Paths with a higher cumulative reward are more likely.

MaxEnt-IRL uses the following linear parametrized representation:
\[
R(s,a) = x^T \theta,
\]
where $x$ is the state vector. The resulting form is:
\[
P(d_i | R) \propto \exp \{ \sum_{t=0}^T x^T \theta \},
\]
and MaxEnt-IRL proposes an algorithm to infer the $\theta$ that maximizes the posterior likelihood.

\hirl applies MaxEnt-IRL to each segment of the task but with a small modification to learn quadratic rewards instead of linear ones. 
Let $\mu_i$ be the centroid of the next transition region.
We want to learn a reward function of the form:
\[
R_i(x) = -(x-\mu_i)^T Q (x-\mu_i).
\]
for a positive semi-definite $Q$ (negated since this is a negative quadratic cost).
With some re-parametrization, this reward function can be written as:
\[
R_i(x) = -\sum_{j=1}^d \sum_{l=1}^d q_{ij} x[j] x[l].
\]
which is linear in the feature-space $y = x[j] x[l]$:
\[
R_i(x) = \theta^T y.
\]

This posterior inference procedure requires a dynamics model. We fit local linear models to each of the segments discovered in the previous section:
\[
A_j = \arg\min_{A} \sum_{i=1}^N \sum_{\text{seg j start}}^{\text{seg j end}} \|A x^{(i)}_{t} - x^{(i)}_{t+1}\|
\]
In this form, the problem can be analytically solved with techniques proposed in~\cite{ziebart2012probabilistic}.
\hirl applies MaxEnt-IRL to the sub-sequences of demonstrations between 0 and $\rho_1$, and then from $\rho_1$ to $\rho_2$ and so on.
The result is an estimated local reward function $R_{i}$ modeled as a linear function of states that is associated with each $\rho_i$.

\iffalse
\subsubsection{Application Scenarios} While typically RL is applied in scenarios where dynamics models are unavailable, there are scenarios in which the model-based reward approach applies. Suppose we are given demonstrations of a known system, e.g., a simulator or a simplified version of a task, with different dynamics $P'$ but with the same overall task structure.
While a policy learned in this setting would not transfer due to a change in dynamics, the reward function would.
In our experiments, we demonstrate an example of this on a surgical cutting task where the robot has to cut along a marked line in gauze.
We demonstrate cutting trajectories on a simplified cutting problem where the demonstrator quasi-statically traces the line without cutting it.
Here there are known dynamics and we can learn transitions and local rewards (e.g., learning to follow the line).
This reward can transfer to the policy learning phase, in which we can try to cut the gauze introducing unknown dynamics.
\fi


\begin{phase}[t]
\small
\DontPrintSemicolon
\caption{Reward Learning \label{alg:tsh2}}
\KwData{Demonstration $\mathcal{D}$ and sub-goals $[\rho_1,...,\rho_k]$}

Based on the transition states, segment each demonstration $d_i$ into $k$ sub-sequences where the $j^{th}$ is denoted by $d_i[j]$.

If dynamics model is available, apply MaxEnt-IRL to each set of sub-sequences $1...k$.

If the dynamics model is not available compute Equation \ref{localq} for each set of subsequences.

\KwResult{$\mathbf{R}_{seq}$}
\end{phase}

\vspace{-15pt}
\subsubsection{Model-free: Local Quadratic Rewards}
When the linearity assumption cannot be made, \hirl can alternatively use a model-free approach for reward construction.
The role of the reward function is to guide the robot to the next transition region $\rho_i$.
A straight forward thing approach is for each segment $i$, we can define a reward function as follows:
\[
R_i(x) = -\|x - \mu_{i}\|_2^2, 
\]
which is just the Euclidean distance to the centroid.

A problem with using Euclidean distance directly is that it uniformly penalizes disagreement with $\mu$ in all dimensions.
During different stages of a task, some features will likely naturally vary more than others--this is learned through IRL.
To account for this, we derive a reasonable $Q$ that is independent of the dynamics:
\[
Q[j,l] = \Sigma^{-1}_x,
\]
which is the inverse of the covariance matrix of all of the state vectors in the segment:
\begin{equation}
Q[j,l] = (\sum_{t=start}^{end} x x^T)^{-1},
\label{localq}
\end{equation}
which is a $p \times p$ matrix defined as the covariance of all of the states in the segment $i-1$ to $i$.
Intuitively, if a feature has low variance during this segment, deviation in that feature from the desired target it gets penalized. 
This is exactly the Mahalonabis distance to the next transition. 

For example, suppose one of the features $j$ measures the distance to a reference trajectory $u_t$. 
Further, suppose in step one of the task the demonstrator's actions are perfectly correlated with the trajectory ($Q_{i}[j,j]$ is low where variance is in the distance) and in step two the actions are uncorrelated with the reference trajectory ($Q_{i}[j,j]$ is high).
Thus, $Q$ will respectively penalize deviation from $\mu_{i}[j]$ more in step one than in step two.


\subsection*{Phase III. Policy Learning}
\seclabel{policy-learning}

In Phase III, \hirl uses the learned transitions $[\rho_1,...,\rho_k]$ and $\mathbf{R}_{seq}$ as rewards for a Reinforcement Learning algorithm.
In this section, we describe learning a policy $\pi$ given rewards $\mathbf{R}_{seq}$ and an ordered sequence of transitions $G$.

However, this problem is not trivial since solving $k$ independent problems neglects potential shared value structure between the local problems (e.g., a common failure state).
Furthermore, simply taking the aggregate of the rewards can lead to inconsistencies since there is nothing enforcing the order of operations.
The key insight is that a single policy can be learned jointly over all segments over a modified problem where the state-space with additional variables that keep track of the previously achieved segments.
To do so, we require an MDP model that also captures the history of the process.


\begin{phase}[t]
\small
\DontPrintSemicolon
\caption{Policy Learning \label{alg:tsh3}}
\KwData{Transition States $G$, Reward Sequence $\mathbf{R}_{seq}$, exploration parameter $\epsilon$}

Initialize $Q(\binom{s}{v},a)$ randomly

\ForEach{$iter \in 0,...,I$}{
    Draw $s_0$ from initial conditions
    
    Initialize $v$ to be $[0,...,0]$
    
    Initialize $j$ to be $1$
    
    \ForEach{$t \in 0,...,T$}{
        Choose best action $a$ based on $Q$ or random action w.p $\epsilon$.
        
        Observe Reward $R_{j}$
        
        Update state to $s'$ and $Q$ via Q-Learning update
        
        If $s'$ is $\in  \rho_{j}$ update $v[j] = 1$ and $j = j +1$
    }
}

\KwResult{Policy $\pi$}
\end{phase}

\vspace{-15pt}
\subsubsection{MDPs with Memory}
RL algorithms apply to problems that are specified as MDPs.
The challenge is that some sequential tasks may not be MDPs.
For example, attaining a reward at $\rho_i$ depends on knowing that the reward at goal $\rho_{i-1}$ was attained.
In general, to model this dependence on the past requires MDPs whose state-space also includes history.

Given a finite-horizon MDP $\mathcal{M}$ as defined in Section \ref{sec:back}, we can define an MDP $\mathcal{M}_H$ as follows.
Let $\mathcal{H}$ denote set of all dynamically feasible sequences of length smaller than $T$ comprised of the elements of $S$.
Therefore, for an agent at any time $t$, there is a sequence of previously visited states $H_t \in \mathcal{H}$.
The MDP $\mathcal{M}_H$ is defined as:
\[
\mathcal{M}_H = \langle S \times \mathcal{H},A,P'(\cdot,\cdot), R(\cdot,\cdot),T \rangle.
\]
For this MDP, $P'$ not only defines the transitions from the current state $s \mapsto s'$, but also increments the history sequence $H_{t+1} = H_{t} \sqcup s$.
Accordingly, the parametrized reward function $R$ is defined over $S$, $A$, and $H_{t+1}$.

$\mathcal{M}_H$ allows us to address the sequentiality problem since the reward is a function of the state and the history sequence.
However, without some parametrization of $H_t$, directly solving this MDPs with RL is impractical since it adds an overhead of $\mathcal{O}(e^{T})$ states.

\vspace{-15pt}
\subsubsection{Policy Learning}
Using our sequential task definition, we know that the reward transitions ($R_{i}$ to $R_{i+1}$) only depend on an arrival at the transition state $\rho_{i}$ and not any other aspect of the history.
Therefore, we can store a vector $v$, a $k$ dimensional binary vector ($v \in \{0,1\}^k$) that indicates whether a transition state $i \in 0,...,k$ has been reached.
This vector can be efficiently incremented when the current state $s \in \rho_{i+1}$.
Then, the additional complexity of representing the reward with history over $S \times  \{0,1\}^k$ is only $\mathcal{O}(k)$ instead of exponential in the time horizon.

The result is an augmented state-space $\binom{s}{v}$ to account for previous progress.
Over this state-space, we can apply Reinforcement Learning algorithms to iteratively converge to a successful policy for a new task instance.
\hirl applies Q-Learning with a Radial Basis Function value function representation to learn a policy $\pi$ over this state-space and the reward sequence $\mathbf{R}_{seq}$.
This is summarized in Algorithm~\ref{alg:tsh3}.
