\section{Sequential Consumable Rewards}\label{sec:seq}
We study tasks where trajectories are constructed from locally linear motions that switch upon receiving rewards.
The main challenge with segmentation is that it introduces sequential dependencies between the rewards.
Segmented rewards are consumable, that is only the agent's first visit to a goal state is rewarded, and conditioned on knowing which previous rewards were received.

\subsection{Sequential Consumable Rewards}
Let $[\rho_1, ...., \rho_k]$ define a sequence of $k$ regions of the state-space $\rho_i \subseteq S$. 
We call each of these $k$ regions a \emph{sub-goal}.
The agent starts at an initial state distribution $p_0$ with a current sub-goal of $\rho_1$. 
Upon reaching any state $s \in \rho_1$, the new sub-goal becomes $\rho_2$, and so on.
The reward achieved by the agent is the total number of sub-goals reached.
Thus, an optimal policy for such a reward is any policy $\pi$ such that the probability $k$ sub-goals are reached is maximized.
The key aspect of this model is that it enforces that the sub-goals are reached in the proper sequence.
To be able to learn such a reward function, we have to be able to infer $\{\rho_1,...,\rho_k\}$ from a set of demonstration trajectories $D$.
This will require a modeling assumption that rewards correlated with changes in agent motion, which is often the case in many physical tasks and Section \ref{sec:control} will provide more information.

It also turns out that such a reward structure is not compatible with the MDP $\mathcal{M}$ since it it has dependence on the past. To model this as a proper MDP, we have to use an MDP whose state-space also include history $\mathcal{M}_H$.
This definition is very useful in understanding the hardness of the general problem, and in Section \ref{sec:hard}, we show that the general problem is in fact computationally hard since identifying optimal features is equivalent to Non-Deterministic Finite Automata minimization.

\subsection{Demonstration Model}
We are given a set of demonstrations $D=\{d_1,...,d_N\}$ that are sequences of states and actions of an optimal policy.
Along with each $d_i$, let $\mathbf{x}_i(t)$ represents a trajectory in feature space $[\mathbf{x}_i(t)= f(s_1,a_1),...,f(s_T,a_T)]$.
We model the agent's trajectory in $\mathbb{R}^p$ as a sequence of variable length locally linear dynamical motions.
These motions are generated by an unknown switching linear dynamical system with zero-mean, normally distributed process noise:
\[
\mathbf{x}(t+1) = A_{i}\mathbf{x}(t) + W(t) \text{ : } A_i \in \{A_1,...,A_m\}.
\]
\emph{Transitions} are defined as times where $A(t) \ne A(t+1)$.
Thus, each transition will have an associated feature value $\mathbf{x}(t)$ called a transition state.
We model these $\mathbf{x}(t)$ as generated from a Gaussian Mixture Model over the feature space $\mathbb{R}^p$ and time (see Krishnan et al. for details~\cite{krishnan2015tsc}).
At first glance, the linearity assumption might seem quite strong, but it turns out that the demonstrations need to be linear in \emph{some} feature-space, which can be achieved with kernel dimensionality reduction (Section \ref{sec:kern}).
 
We interpret this mixture model as defining sub-goals for the task.
If there $k$ mixture components for the distribution $\{m_1,...,m_k\}$, the $\alpha$-quantile of each component distribution will define a region $\{\rho_1,...,\rho_k\}$ over the feature space at some time-interval.
We interpet the learned $\{\rho_1,...,\rho_k\}$ as the sub-goals.
As in the model, rewards for the intermediate goals are earned at the first entry into each region only if all previous regions have been rewarded.

\subsection{Problem Statements}
In this paper, we address the following problems:

\vspace{0.5em}

\noindent\textbf{Transition State Problem: } Given a set of demonstrations $D$ and a featurization $f$, identify $\{\rho_1,...,\rho_k\}$.

\vspace{0.5em}

\noindent\textbf{Segment Encoding Problem: } 
At any time $t$, let $H_t$ denote the sequence of previously visited states by an agent.
Using $\{\rho_1,...,\rho_k\}$, we construct a function $e(\cdot)$ that maps $H_t$ to a $k$-dimensional feature vector $v$ recording previously achieved sub-goals. 
We want to find a $k$-dimensional vector that most accurately represents the history of a process with $k$ transition states.

\vspace{0.5em}

\noindent\textbf{Reward Function Construction: }
We will use $v$ to construct reward functions for the sequential task.
We explore using quadratic distance-to-state rewards and integrating this vector with IRL.
$v$ will augment the state-space $S$, and we use Maximum Entropy Inverse Reinforcement Learning to learn a reward function across the feature space $\mathbb{R}^p$ and $v$. 
To use this reward function in Forward RL, the state-space is augmented with $v$.