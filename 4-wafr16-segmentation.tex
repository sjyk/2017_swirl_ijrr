\section{\hirlfull}
\seclabel{hirl}
This section describes an algorithm to infer the parameters for the proposed model.

\subsubsection{Algorithm Description}
Let $D$ be a set of demonstration trajectories $\{d_1,...,d_N\}$ of a task with a delayed reward.
\hirl can be described in terms of three sub-algorithms:

\vspace{2pt}
\noindent\textbf{Inputs:} Demonstrations $D$
\begin{enumerate}[
    topsep=0pt,
    noitemsep,
    % partopsep=1ex,
    % parsep=1ex,
    leftmargin=*,
    % itemindent=3ex
    ]
    \item \textbf{Sequence Learning: } Given $D$, \hirl segments the task into $k$ sub-tasks whose start and end are defined by arrival at a set of transitions $G = [\rho_1,...,\rho_k]$.
    \item \textbf{Reward Learning: } Given $G$ and $D$, \hirl associates a local reward function with the segment resulting in a sequence of rewards $\mathbf{R}_{seq}$. 
    \item \textbf{Policy Learning: } Given $\mathbf{R}_{seq}$ and $G$, \hirl applies reinforcement learning for $I$ iterations to learn a policy for the task $\pi$.
\end{enumerate}

\noindent\textbf{Outputs:} Policy $\pi$


\subsection*{Phase I. Sequence Learning}
\seclabel{algo}
The first phase of \hirl is to segment the demonstrations into locally linear segments.
We can then cluster the segment endpoints into $k$ clusters to infer  $[\rho_1,...,\rho_k]$. This is an extension of our prior work on robust task segmentation~\cite{krishnan2015tsc,murali2016}.
The overall procedure is summarized in Phase I.

\subsubsection{Segmentation and Transition Clustering}
The first step is given a set of demonstration trajectories, decompose each trajectory into segments.
A popular approach for segmentation is to use Gaussian Mixture Models~\cite{calinon2014skills}, namely, cluster all state observations and identify times at which $x_t$ is in a different cluster than $x_{t+1}$.
For a given time $t$, we can define a window of length $\ell$ as:
\[
\mathbf{n}^{(\ell)}_t = [x_{t-\ell},...,x_{t}]^\intercal
\]
Then, for each demonstration trajectory we can also generate a trajectory of $T_i - \ell$ windowed states:
\[
\mathbf{d}^{(\ell)}_i = [\mathbf{n}^{(\ell)}_\ell,...,\mathbf{n}^{(\ell)}_{T_i}]
\]
Over the entire set of windowed demonstrations, we collect a dataset of all of the $\mathbf{n}^{(\ell)}_t$ vectors.
We fit GMM model to these vectors.
The GMM model defines $m$ multivariate Gaussian distributions and a probability that each observation $\mathbf{n}^{(\ell)}_t$ is sampled from each of the $m$ distributions.
We annotate each observation with the most likely mixture component.
Times such that $\mathbf{n}^{(\ell)}_t$ and $\mathbf{n}^{(\ell)}_{t+1}$ have different most likely components are marked as transitions.
This has the interpretation of fitting a locally linear regression to the data (refer to~\cite{moldovan2013dirichlet, khansari2011learning, kruger2010learning, krishnan2015tsc,murali2016} for details).
In typical GMM formulations, one must specify the number of mixture components $k$ before hand.
However, we apply results from Bayesian non-parametric statistics and jointly solve for the component parameters and the number of components using a Dirichlet Process~\cite{kulis2011revisiting}.
Using a DP, the number of components grows with the complexity of the observed data (we denote this as DP-GMM).


In prior work, we noticed that motion-based segmentation algorithms can be unreliable when there is noise~\cite{krishnan2015tsc}.
We, however, realized that applying a second level of cluster--clustering the segment endpoints found dense clusters of common transitions that occurred in all demonstrations--thus allowing us to reject spurious motions or observation noise.
The insight of this work is that the same approach can be interpreted as identifying necessary sub-goals in a complex task.

We would like to be able to aggregate the transition times into state-space conditions for reward transitions $[\rho_1,...,\rho_k]$.
To each of these transition times, there is a corresponding \emph{transition state}--the last state before the dynamics switched.
We can model these regions againwith a Gaussian Mixture Model with $k$ mixture components $\{m_1,...,m_k\}$.
As before, we use a DP to non-parametrically set $k$.
Then, we prune clusters that do not have at least one transition from each demonstration.
Thus, the result is the set of transition regions: $G = [\rho_1, \rho_2,...,\rho_k]$, and segmentation of each demonstration trajectory into $k$ segments.

\subsubsection{Relaxing Local Linearity}
GMM's are a type of local Bayesian linear regression, but we can easily relax the linearity assumption.
We relax the linear dynamics assumption with a kernel embedding of the trajectories.
\hirl does not require learning the exact regimes $A_i$,  it only needs to detect changes in dynamics regime.
The basic idea is to apply Kernelized PCA to the features before learning the transitions--a technique used in Computer Vision~\cite{DBLP:conf/nips/MikaSSMSR98}.
By changing the kernel function (i.e., the similarity metric between states), we can essentially change the definition of local linearity.

Let $\mathbf{\kappa}(x_i,x_j)$ define a kernel function over the states.
For example, if $\mathbf{\kappa}$ is the radial basis function (RBF), then:
$ \mathbf{\kappa}(x_i,x_j) = e^{\frac{-\|x_i-x_j\|_2^2}{2\sigma}}$.
$\mathbf{\kappa}$ naturally defines a matrix $M$ where: $M_{ij} = \mathbf{\kappa}(x_i,x_j)$. 
The top $p'$ eigenvalues define a new embedded feature vector for each $\omega$ in $\mathbb{R}^{p'}$.
We can now apply the algorithm above in this embedded feature space.

\begin{phase}[t]
\small
\DontPrintSemicolon
\caption{Sequence Learning \label{alg:tsh1}}
\KwData{Demonstration $\mathcal{D}$}

Fit a DP-GMM model to $\mathcal{D}$ and identify the set of transitions $\Theta$, defined as all $(x_t,t)$  where  $(x_{t+1},t+1)$  has a different cluster.

Fit a DP-GMM to the states in $\Theta$.

Prune clusters that do not have one transition from all demonstrations.

The result of is $G = [\rho_1, \rho_2,...,\rho_m]$ where each $\rho$ is a disjoint ellipsoidal region of the state-space and time interval.

\KwResult{G}
\end{phase}
